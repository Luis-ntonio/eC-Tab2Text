{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.litellm import LiteLLM\n",
    "from prometheus_eval.prompts import ABSOLUTE_PROMPT, SCORE_RUBRIC_TEMPLATE\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Outputs/StructLm-7B/StructLM-7B-M1-D1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#folder = 'Gemini_1.5_Flash'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#folder = 'Mistral-7B-Instruct-v0.3'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStructLm-7B\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Outputs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Outputs/StructLm-7B/StructLM-7B-M1-D1.json'"
     ]
    }
   ],
   "source": [
    "#name = 'Gemini_1.5_Flash-raw'\n",
    "#name = 'Mistral-7B-Instruct-v0.3_raw'\n",
    "name = 'StructLM-7B-M1-D1'\n",
    "\n",
    "#folder = 'Gemini_1.5_Flash'\n",
    "#folder = 'Mistral-7B-Instruct-v0.3'\n",
    "folder = 'StructLm-7B'\n",
    "\n",
    "with open(f\"./Outputs/{folder}/{name}.json\", 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(org, pred, metric_, value_):\n",
    "    dict_metric = {}\n",
    "    metric = evaluate.load(metric_)\n",
    "    metric_per_review = 0\n",
    "    idx = 0\n",
    "    if \"M1-D1\" in name or 'raw' in name:\n",
    "        for i in tqdm(range(len(org))):\n",
    "            review_org = org[i]\n",
    "            review_pred = pred[i]\n",
    "            try:\n",
    "                for key, val in review_org.items():\n",
    "                    if key not in dict_metric:\n",
    "                        dict_metric[key] = [0, 0] \n",
    "                    val_pred = [str(review_pred[key])]\n",
    "                    if metric_ == 'bleu':        \n",
    "                        score = metric.compute(references=[str(val)], predictions=val_pred, max_order=2, smooth=True)[value_]\n",
    "                    elif metric_ == 'bertscore':\n",
    "                        score = metric.compute(references=[str(val)], predictions=val_pred, lang=\"en\")[\"f1\"][0]\n",
    "                    else:\n",
    "                        score = metric.compute(references=[str(val)], predictions=val_pred)[value_]\n",
    "                    dict_metric[key][0] += score\n",
    "                    dict_metric[key][1] += 1\n",
    "                    idx += 1\n",
    "            except:\n",
    "                idx += 1\n",
    "        sum = 0\n",
    "        rest = 0\n",
    "        for key in dict_metric.keys():\n",
    "            if dict_metric[key][1] != 0:\n",
    "                dict_metric[key][0] = round((dict_metric[key][0]/dict_metric[key][1])*100, 2)\n",
    "            else:\n",
    "                rest += 1\n",
    "            sum += dict_metric[key][0]\n",
    "        if len(dict_metric.keys()) - rest == 0:\n",
    "\n",
    "            dict_metric[\"Total\"] = [0]\n",
    "        dict_metric[\"Total\"] = [round(sum/(len(dict_metric.keys()) - rest), 2), len(dict_metric.keys()) - rest]\n",
    "        return dict_metric\n",
    "    else:\n",
    "        score = 0\n",
    "        for i in tqdm(range(len(org))):\n",
    "            if metric_ == 'bleu':        \n",
    "                score += metric.compute(references=[str(org[i])], predictions=[str(pred[i])], max_order=2, smooth=True)[value_]\n",
    "            elif metric_ == 'bertscore':\n",
    "                score = metric.compute(references=[str(org[i])], predictions=[str(pred[i])], lang=\"en\")[\"f1\"][0]\n",
    "            else:\n",
    "                score += metric.compute(references=[str(org[i])], predictions=[str(pred[i])])[value_]\n",
    "        \n",
    "        dict_metric[\"Total\"] = [round(score/(len(org)) * 100, 2), len(org)]\n",
    "        return dict_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1026 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1026/1026 [00:12<00:00, 81.72it/s]\n"
     ]
    }
   ],
   "source": [
    "Original = data['Original']\n",
    "Pred = data['Prediction']\n",
    "bertscore = evaluating(Original, Pred, 'bertscore', 'bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original = data[\\'Original\\']\\nPred = data[\\'Prediction\\']\\nbleu = evaluating(Original, Pred, \\'bleu\\', \\'bleu\\')\\nrougeL = evaluating(Original, Pred, \"rouge\", \"rougeL\")\\nrouge1 = evaluating(Original, Pred, \"rouge\", \"rouge1\")\\nrouge2 = evaluating(Original, Pred, \"rouge\", \"rouge2\")\\nrougeLsum = evaluating(Original, Pred, \"rouge\", \"rougeLsum\")\\nmeteor = evaluating(Original, Pred, \"meteor\", \"meteor\")'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Original = data['Original']\n",
    "Pred = data['Prediction']\n",
    "bleu = evaluating(Original, Pred, 'bleu', 'bleu')\n",
    "rougeL = evaluating(Original, Pred, \"rouge\", \"rougeL\")\n",
    "rouge1 = evaluating(Original, Pred, \"rouge\", \"rouge1\")\n",
    "rouge2 = evaluating(Original, Pred, \"rouge\", \"rouge2\")\n",
    "rougeLsum = evaluating(Original, Pred, \"rouge\", \"rougeLsum\")\n",
    "meteor = evaluating(Original, Pred, \"meteor\", \"meteor\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(f\"BLEU: {bleu}\")\\nprint(f\"ROUGE-L: {rougeL}\")\\nprint(f\"ROUGE-1: {rouge1}\")\\nprint(f\"ROUGE-2: {rouge2}\")\\nprint(f\"ROUGE-Lsum: {rougeLsum}\")\\nprint(f\"METEOR: {meteor}\")'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(f\"BLEU: {bleu}\")\n",
    "print(f\"ROUGE-L: {rougeL}\")\n",
    "print(f\"ROUGE-1: {rouge1}\")\n",
    "print(f\"ROUGE-2: {rouge2}\")\n",
    "print(f\"ROUGE-Lsum: {rougeLsum}\")\n",
    "print(f\"METEOR: {meteor}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data['rougeL'] = rougeL\\ndata['rouge1'] = rouge1\\ndata['rouge2'] = rouge2\\ndata['rougeLsum'] = rougeLsum\\ndata['meteor'] = meteor\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(f\"./metrics/{folder}/{name}.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data['bertscore'] = bertscore\n",
    "\"\"\"data['rougeL'] = rougeL\n",
    "data['rouge1'] = rouge1\n",
    "data['rouge2'] = rouge2\n",
    "data['rougeLsum'] = rougeLsum\n",
    "data['meteor'] = meteor\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./metrics/{folder}/{name}.json\", 'w+') as f:\n",
    "    json.dump(data, f, indent=4 ,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394aae8bff634db5ac42d29496e030e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87352bf1f9b405f93af94d10c2f5dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"prometheus-eval/prometheus-7b-v2.0\"\n",
    "\n",
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad_token_id to eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corectness(Prompt, Predicted, Original):\n",
    "    instruction = f\"\"\"Your task is to evaluate the generated answer and reference answer for the query: {Prompt}\"\"\"\n",
    "    response = f\"\"\"{Predicted}\"\"\"\n",
    "    reference_answer = f\"\"\"{Original}\"\"\"\n",
    "    rubric = {\n",
    "            \"criteria\": \"Is the model proficient in generate a coherence response\",\n",
    "            \"score1_description\": \"If the generated answer is not matching with any of the reference answers.\",\n",
    "            \"score2_description\": \"If the generated answer is according to reference answer but not relevant to user query.\",\n",
    "            \"score3_description\": \"If the generated answer is relevant to the user query and reference answer but contains mistakes.\",\n",
    "    \t\t\"score4_description\": \"If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\",\n",
    "            \"score5_description\": \"If the generated answer is relevant to the user query and fully correct according to the reference answer.\"}\n",
    "    #https://github.com/prometheus-eval/prometheus-eval\n",
    "\n",
    "    ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "\n",
    "    ABSOLUTE_PROMPT = f\"\"\"###Task Description:\n",
    "    An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "    1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "    2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "    3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "    4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "    ###The instruction to evaluate:\n",
    "    {instruction}\n",
    "\n",
    "    ###Response to evaluate:\n",
    "    {response}\n",
    "\n",
    "    ###Reference Answer (Score 5):\n",
    "    {reference_answer}\n",
    "\n",
    "    ###Score Rubrics:\n",
    "    {rubric}\n",
    "\n",
    "    ###Feedback: \"\"\"\n",
    "\n",
    "    user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT # Fill the prompt with your data\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs = encodeds.to(\"cuda\")\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=4000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faithfullness(Prompt, Predicted, Original):\n",
    "    instruction = f\"\"\"Your task is to evaluate if the Generate answer has information from the context and also from the Existing answer.\"\"\"\n",
    "    response = f\"\"\"{Predicted}\"\"\"\n",
    "    reference_answer = f\"\"\"{Original}\"\"\"\n",
    "    rubric = {\n",
    "            \"score1_description\": \"If the generated answer is not matching with any of the reference answers and also not having information from the context.\",\n",
    "            \"score2_description\": \"If the generated answer is having information from the context but not from existing answer and also have some irrelevant information.\",\n",
    "            \"score3_description\": \"If the generated answer is having relevant information from the context and some information from existing answer but have additional information that do not exist in context and also do not in existing answer.\",\n",
    "    \t\t\"score4_description\": \"If the generated answer is having relevant information from the context and some information from existing answer.\",\n",
    "            \"score5_description\": \"If the generated answer is matching with the existing answer and also having information from the context.\"}\n",
    "    #https://github.com/prometheus-eval/prometheus-eval\n",
    "\n",
    "    ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "\n",
    "    ABSOLUTE_PROMPT = f\"\"\"###Task Description:\n",
    "    An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "    1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "    2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "    3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "    4. Please do not generate any other opening, closing, and explanations.\n",
    "    5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n",
    "\n",
    "    ###The instruction to evaluate:\n",
    "    {instruction}\n",
    "\n",
    "    ###Context:\n",
    "    {Prompt}\n",
    "\n",
    "    ###Existing answer (Score 5):\n",
    "    {reference_answer}\n",
    "\n",
    "    ###Generate answer to evaluate:\n",
    "    {response}\n",
    "\n",
    "    ###Score Rubrics:\n",
    "    {rubric}\n",
    "\n",
    "    ###Feedback: \"\"\"\n",
    "\n",
    "    user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT # Fill the prompt with your data\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs = encodeds.to(\"cuda\")\n",
    "\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=4000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluency(Prompt, Predicted, Original):\n",
    "    instruction = f\"\"\"Evaluate the fluency of the generated JSON answer.\"\"\"\n",
    "    response = json.dumps(Predicted, indent=2)\n",
    "    reference_answer = json.dumps(Original, indent=2)\n",
    "    rubric = {\n",
    "            \"score1_description\": \"The generated JSON answer is not fluent and is difficult to understand.\",\n",
    "            \"score2_description\": \"The generated JSON answer has several grammatical errors and awkward phrasing.\",\n",
    "            \"score3_description\": \"The generated JSON answer is mostly fluent but contains some grammatical errors or awkward phrasing.\",\n",
    "            \"score4_description\": \"The generated JSON answer is fluent with minor grammatical errors or awkward phrasing.\",\n",
    "            \"score5_description\": \"The generated JSON answer is perfectly fluent with no grammatical errors or awkward phrasing.\"}\n",
    "    #https://github.com/prometheus-eval/prometheus-eval\n",
    "\n",
    "    ABS_SYSTEM_PROMPT = \"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\"\n",
    "\n",
    "    ABSOLUTE_PROMPT = f\"\"\"###Task Description:\n",
    "    An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "    1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "    2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "    3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "    4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "    ###The instruction to evaluate:\n",
    "    {instruction}\n",
    "\n",
    "    ###Response to evaluate:\n",
    "    {response}\n",
    "\n",
    "    ###Reference Answer (Score 5):\n",
    "    {reference_answer}\n",
    "\n",
    "    ###Score Rubrics:\n",
    "    {rubric}\n",
    "\n",
    "    ###Feedback: \"\"\"\n",
    "\n",
    "    user_content = ABS_SYSTEM_PROMPT + \"\\n\\n\" + ABSOLUTE_PROMPT # Fill the prompt with your data\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ]\n",
    "\n",
    "    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "    model_inputs = encodeds.to(\"cuda\")\n",
    "\n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=4000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    return decoded[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2239 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 2239/2239 [3:02:39<00:00,  4.89s/it]  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(len(data[\"Original\"]))):\n",
    "    Prompt = data[\"Prompt\"][i]\n",
    "    Pred = data[\"Prediction\"][i]\n",
    "    Org = data[\"Original\"][i]\n",
    "    resp = corectness(Prompt=Prompt, Predicted=Pred, Original=Org)\n",
    "    results.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2239/2239 [3:10:01<00:00,  5.09s/it]  \n"
     ]
    }
   ],
   "source": [
    "results_faithfullness = []\n",
    "for i in tqdm(range(len(data[\"Original\"]))):\n",
    "    Prompt = data[\"Prompt\"][i]\n",
    "    Pred = data[\"Prediction\"][i]\n",
    "    Org = data[\"Original\"][i]\n",
    "    resp = faithfullness(Prompt=Prompt, Predicted=Pred, Original=Org)\n",
    "    results_faithfullness.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2239/2239 [2:55:30<00:00,  4.70s/it]  \n"
     ]
    }
   ],
   "source": [
    "results_fluency = []\n",
    "for i in tqdm(range(len(data[\"Original\"]))):\n",
    "    Prompt = data[\"Prompt\"][i]\n",
    "    Pred = data[\"Prediction\"][i]\n",
    "    Org = data[\"Original\"][i]\n",
    "    resp = corectness(Prompt=Prompt, Predicted=Pred, Original=Org)\n",
    "    results_fluency.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ = []\n",
    "for i in results:\n",
    "    try:\n",
    "        val = i.split(\"[RESULT] \")\n",
    "        val = val[2]\n",
    "        results_.append(int(val[0]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "results_faithfullness_ = []\n",
    "for i in results_faithfullness:\n",
    "    try:\n",
    "        val = i.split(\"[RESULT] \")\n",
    "        val = val[2]\n",
    "        results_faithfullness_.append(int(val[0]))\n",
    "    except:\n",
    "        pass\n",
    "results_fluency_ = []\n",
    "for i in results_fluency:\n",
    "    try:\n",
    "        val = i.split(\"[RESULT] \")\n",
    "        val = val[2]\n",
    "        results_fluency_.append(int(val[0]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"bleu\" : bleu,\n",
    "    \"rouge1\" : rouge1,\n",
    "    \"rouge2\" : rouge2,\n",
    "    \"rougeL\" : rougeL,\n",
    "    \"rougeLsum\" : rougeLsum,\n",
    "    \"meteor\" : meteor,\n",
    "    \"correctness\": ((sum(results_)/len(results_)) * 100)/5,\n",
    "    \"faithfullness\": ((sum(results_faithfullness_)/len(results_faithfullness_)) * 100)/5,\n",
    "    \"fluency\": ((sum(results_fluency_)/len(results_fluency_)) * 100)/5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.79410451094239\n"
     ]
    }
   ],
   "source": [
    "print(metrics[\"correctness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./metrics/{folder}/{name}.json\", 'w+') as f:\n",
    "    json.dump(metrics, f, indent=4 ,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
