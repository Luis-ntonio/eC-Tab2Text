{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "dataset_name = \"kokujin/prompts_1\"\n",
    "\n",
    "new_model = \"Mistral-7B-Instruct-v0.3-luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{model_name}/\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 3\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 1000\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d8a73a7fa844f4890d81c5074b497c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0acff98277045be8dcc1951c15ec8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0361, 'grad_norm': 0.3715257942676544, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 0.8629, 'grad_norm': 0.9048253893852234, 'learning_rate': 5.208333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3976, 'grad_norm': 0.626203715801239, 'learning_rate': 7.8125e-05, 'epoch': 0.02}\n",
      "{'loss': 0.272, 'grad_norm': 1.2425174713134766, 'learning_rate': 0.00010416666666666667, 'epoch': 0.03}\n",
      "{'loss': 0.204, 'grad_norm': 0.34642747044563293, 'learning_rate': 0.00013020833333333333, 'epoch': 0.04}\n",
      "{'loss': 0.1951, 'grad_norm': 0.9093351364135742, 'learning_rate': 0.00015625, 'epoch': 0.05}\n",
      "{'loss': 0.1572, 'grad_norm': 0.2559562921524048, 'learning_rate': 0.00018229166666666667, 'epoch': 0.05}\n",
      "{'loss': 0.2016, 'grad_norm': 0.42944344878196716, 'learning_rate': 0.00019999917944905216, 'epoch': 0.06}\n",
      "{'loss': 0.1573, 'grad_norm': 0.23466379940509796, 'learning_rate': 0.00019998603811858571, 'epoch': 0.07}\n",
      "{'loss': 0.1921, 'grad_norm': 0.3496451675891876, 'learning_rate': 0.00019995687283209658, 'epoch': 0.08}\n",
      "{'loss': 0.1386, 'grad_norm': 0.2484898716211319, 'learning_rate': 0.00019991168826367011, 'epoch': 0.09}\n",
      "{'loss': 0.1586, 'grad_norm': 0.29783809185028076, 'learning_rate': 0.00019985049165467257, 'epoch': 0.09}\n",
      "{'loss': 0.1272, 'grad_norm': 0.1712491810321808, 'learning_rate': 0.00019977329281259108, 'epoch': 0.1}\n",
      "{'loss': 0.1653, 'grad_norm': 0.3634892702102661, 'learning_rate': 0.00019968010410946154, 'epoch': 0.11}\n",
      "{'loss': 0.1241, 'grad_norm': 0.18681398034095764, 'learning_rate': 0.00019957094047988582, 'epoch': 0.12}\n",
      "{'loss': 0.1393, 'grad_norm': 0.3389054536819458, 'learning_rate': 0.00019944581941863857, 'epoch': 0.13}\n",
      "{'loss': 0.1172, 'grad_norm': 0.22925116121768951, 'learning_rate': 0.00019930476097786327, 'epoch': 0.13}\n",
      "{'loss': 0.1513, 'grad_norm': 0.4825553596019745, 'learning_rate': 0.0001991477877638587, 'epoch': 0.14}\n",
      "{'loss': 0.1237, 'grad_norm': 0.2592446804046631, 'learning_rate': 0.00019897492493345598, 'epoch': 0.15}\n",
      "{'loss': 0.1357, 'grad_norm': 0.2628246247768402, 'learning_rate': 0.00019878620018998696, 'epoch': 0.16}\n",
      "{'loss': 0.1091, 'grad_norm': 0.20936407148838043, 'learning_rate': 0.00019858164377884436, 'epoch': 0.16}\n",
      "{'loss': 0.1337, 'grad_norm': 0.1933692991733551, 'learning_rate': 0.00019836128848263465, 'epoch': 0.17}\n",
      "{'loss': 0.1028, 'grad_norm': 0.27440977096557617, 'learning_rate': 0.0001981251696159241, 'epoch': 0.18}\n",
      "{'loss': 0.1333, 'grad_norm': 0.27333107590675354, 'learning_rate': 0.00019787332501957941, 'epoch': 0.19}\n",
      "{'loss': 0.1097, 'grad_norm': 0.18744760751724243, 'learning_rate': 0.0001976057950547031, 'epoch': 0.2}\n",
      "{'loss': 0.145, 'grad_norm': 0.18898768723011017, 'learning_rate': 0.00019732262259616523, 'epoch': 0.2}\n",
      "{'loss': 0.095, 'grad_norm': 0.21582496166229248, 'learning_rate': 0.0001970238530257322, 'epoch': 0.21}\n",
      "{'loss': 0.1054, 'grad_norm': 0.14941897988319397, 'learning_rate': 0.00019670953422479368, 'epoch': 0.22}\n",
      "{'loss': 0.0847, 'grad_norm': 0.18502603471279144, 'learning_rate': 0.00019637971656668918, 'epoch': 0.23}\n",
      "{'loss': 0.12, 'grad_norm': 0.3501814305782318, 'learning_rate': 0.0001960344529086351, 'epoch': 0.23}\n",
      "{'loss': 0.0965, 'grad_norm': 0.20063604414463043, 'learning_rate': 0.00019567379858325356, 'epoch': 0.24}\n",
      "{'loss': 0.1151, 'grad_norm': 0.30999451875686646, 'learning_rate': 0.000195297811389705, 'epoch': 0.25}\n",
      "{'loss': 0.0952, 'grad_norm': 0.22625909745693207, 'learning_rate': 0.00019490655158442484, 'epoch': 0.26}\n",
      "{'loss': 0.1085, 'grad_norm': 0.30245867371559143, 'learning_rate': 0.00019450008187146684, 'epoch': 0.27}\n",
      "{'loss': 0.0977, 'grad_norm': 0.3024768829345703, 'learning_rate': 0.00019407846739245415, 'epoch': 0.27}\n",
      "{'loss': 0.1109, 'grad_norm': 0.20998148620128632, 'learning_rate': 0.00019364177571613926, 'epoch': 0.28}\n",
      "{'loss': 0.0867, 'grad_norm': 0.17439192533493042, 'learning_rate': 0.00019319007682757556, 'epoch': 0.29}\n",
      "{'loss': 0.0938, 'grad_norm': 0.2674555778503418, 'learning_rate': 0.0001927234431169014, 'epoch': 0.3}\n",
      "{'loss': 0.0737, 'grad_norm': 0.1859060376882553, 'learning_rate': 0.00019224194936773853, 'epoch': 0.3}\n",
      "{'loss': 0.0828, 'grad_norm': 0.32896485924720764, 'learning_rate': 0.00019174567274520728, 'epoch': 0.31}\n",
      "{'loss': 0.0664, 'grad_norm': 0.15164466202259064, 'learning_rate': 0.00019123469278355985, 'epoch': 0.32}\n",
      "{'loss': 0.0875, 'grad_norm': 0.2480175644159317, 'learning_rate': 0.00019070909137343408, 'epoch': 0.33}\n",
      "{'loss': 0.0661, 'grad_norm': 0.2269286960363388, 'learning_rate': 0.0001901689527487294, 'epoch': 0.34}\n",
      "{'loss': 0.0854, 'grad_norm': 0.21628433465957642, 'learning_rate': 0.0001896143634731074, 'epoch': 0.34}\n",
      "{'loss': 0.0765, 'grad_norm': 0.27746009826660156, 'learning_rate': 0.00018904541242611902, 'epoch': 0.35}\n",
      "{'loss': 0.0971, 'grad_norm': 0.259750097990036, 'learning_rate': 0.00018846219078896037, 'epoch': 0.36}\n",
      "{'loss': 0.0724, 'grad_norm': 0.18180161714553833, 'learning_rate': 0.00018786479202986006, 'epoch': 0.37}\n",
      "{'loss': 0.0914, 'grad_norm': 0.25816211104393005, 'learning_rate': 0.0001872533118890997, 'epoch': 0.38}\n",
      "{'loss': 0.0683, 'grad_norm': 0.20802262425422668, 'learning_rate': 0.00018662784836367028, 'epoch': 0.38}\n",
      "{'loss': 0.0879, 'grad_norm': 0.24180841445922852, 'learning_rate': 0.00018598850169156722, 'epoch': 0.39}\n",
      "{'loss': 0.0677, 'grad_norm': 0.16083268821239471, 'learning_rate': 0.00018533537433572581, 'epoch': 0.4}\n",
      "{'loss': 0.084, 'grad_norm': 0.24377408623695374, 'learning_rate': 0.00018466857096760046, 'epoch': 0.41}\n",
      "{'loss': 0.0732, 'grad_norm': 0.2606008052825928, 'learning_rate': 0.00018398819845038972, 'epoch': 0.41}\n",
      "{'loss': 0.0889, 'grad_norm': 0.43135368824005127, 'learning_rate': 0.0001832943658219103, 'epoch': 0.42}\n",
      "{'loss': 0.0664, 'grad_norm': 0.2756401002407074, 'learning_rate': 0.00018258718427712238, 'epoch': 0.43}\n",
      "{'loss': 0.0808, 'grad_norm': 0.36994820833206177, 'learning_rate': 0.00018186676715030924, 'epoch': 0.44}\n",
      "{'loss': 0.0641, 'grad_norm': 0.17359623312950134, 'learning_rate': 0.0001811332298969142, 'epoch': 0.45}\n",
      "{'loss': 0.0933, 'grad_norm': 0.27655839920043945, 'learning_rate': 0.0001803866900750376, 'epoch': 0.45}\n",
      "{'loss': 0.0655, 'grad_norm': 0.14252501726150513, 'learning_rate': 0.0001796272673265963, 'epoch': 0.46}\n",
      "{'loss': 0.0717, 'grad_norm': 0.18642756342887878, 'learning_rate': 0.00017885508335815014, 'epoch': 0.47}\n",
      "{'loss': 0.0646, 'grad_norm': 0.265768438577652, 'learning_rate': 0.0001780702619213967, 'epoch': 0.48}\n",
      "{'loss': 0.0744, 'grad_norm': 0.26595669984817505, 'learning_rate': 0.0001772729287933387, 'epoch': 0.48}\n",
      "{'loss': 0.0661, 'grad_norm': 0.1489591896533966, 'learning_rate': 0.00017646321175612668, 'epoch': 0.49}\n",
      "{'loss': 0.0854, 'grad_norm': 0.36603328585624695, 'learning_rate': 0.00017564124057658056, 'epoch': 0.5}\n",
      "{'loss': 0.0533, 'grad_norm': 0.18838918209075928, 'learning_rate': 0.00017480714698539266, 'epoch': 0.51}\n",
      "{'loss': 0.0732, 'grad_norm': 0.387383371591568, 'learning_rate': 0.00017396106465601663, 'epoch': 0.52}\n",
      "{'loss': 0.0635, 'grad_norm': 0.15802884101867676, 'learning_rate': 0.0001731031291832444, 'epoch': 0.52}\n",
      "{'loss': 0.0794, 'grad_norm': 0.2513883709907532, 'learning_rate': 0.0001722334780614756, 'epoch': 0.53}\n",
      "{'loss': 0.0566, 'grad_norm': 0.19691778719425201, 'learning_rate': 0.00017135225066268255, 'epoch': 0.54}\n",
      "{'loss': 0.0823, 'grad_norm': 0.3959238529205322, 'learning_rate': 0.00017045958821407405, 'epoch': 0.55}\n",
      "{'loss': 0.066, 'grad_norm': 0.1837705373764038, 'learning_rate': 0.00016955563377546207, 'epoch': 0.55}\n",
      "{'loss': 0.075, 'grad_norm': 0.26655009388923645, 'learning_rate': 0.0001686405322163349, 'epoch': 0.56}\n",
      "{'loss': 0.0505, 'grad_norm': 0.20359095931053162, 'learning_rate': 0.00016771443019263983, 'epoch': 0.57}\n",
      "{'loss': 0.0714, 'grad_norm': 0.33158689737319946, 'learning_rate': 0.00016677747612327997, 'epoch': 0.58}\n",
      "{'loss': 0.055, 'grad_norm': 0.14876769483089447, 'learning_rate': 0.00016582982016632818, 'epoch': 0.59}\n",
      "{'loss': 0.0643, 'grad_norm': 0.19468384981155396, 'learning_rate': 0.00016487161419496263, 'epoch': 0.59}\n",
      "{'loss': 0.0483, 'grad_norm': 0.24525055289268494, 'learning_rate': 0.00016390301177312722, 'epoch': 0.6}\n",
      "{'loss': 0.08, 'grad_norm': 0.22415503859519958, 'learning_rate': 0.00016292416813092105, 'epoch': 0.61}\n",
      "{'loss': 0.0612, 'grad_norm': 0.28034576773643494, 'learning_rate': 0.00016193524013972114, 'epoch': 0.62}\n",
      "{'loss': 0.0764, 'grad_norm': 0.40281274914741516, 'learning_rate': 0.00016093638628704167, 'epoch': 0.63}\n",
      "{'loss': 0.045, 'grad_norm': 0.24748602509498596, 'learning_rate': 0.0001599277666511347, 'epoch': 0.63}\n",
      "{'loss': 0.0676, 'grad_norm': 0.28065794706344604, 'learning_rate': 0.00015890954287533555, 'epoch': 0.64}\n",
      "{'loss': 0.0464, 'grad_norm': 0.15673571825027466, 'learning_rate': 0.00015788187814215764, 'epoch': 0.65}\n",
      "{'loss': 0.0736, 'grad_norm': 0.2873454689979553, 'learning_rate': 0.00015684493714714047, 'epoch': 0.66}\n",
      "{'loss': 0.0411, 'grad_norm': 0.22378423810005188, 'learning_rate': 0.00015579888607245517, 'epoch': 0.66}\n",
      "{'loss': 0.0733, 'grad_norm': 0.27076515555381775, 'learning_rate': 0.000154743892560272, 'epoch': 0.67}\n",
      "{'loss': 0.0418, 'grad_norm': 0.1413268744945526, 'learning_rate': 0.00015368012568589342, 'epoch': 0.68}\n",
      "{'loss': 0.0639, 'grad_norm': 0.3359984755516052, 'learning_rate': 0.00015260775593065802, 'epoch': 0.69}\n",
      "{'loss': 0.0419, 'grad_norm': 0.17849086225032806, 'learning_rate': 0.00015152695515461865, 'epoch': 0.7}\n",
      "{'loss': 0.0704, 'grad_norm': 0.23173880577087402, 'learning_rate': 0.00015043789656899988, 'epoch': 0.7}\n",
      "{'loss': 0.0393, 'grad_norm': 0.1129363551735878, 'learning_rate': 0.00014934075470843887, 'epoch': 0.71}\n",
      "{'loss': 0.064, 'grad_norm': 0.27609822154045105, 'learning_rate': 0.00014823570540301408, 'epoch': 0.72}\n",
      "{'loss': 0.0464, 'grad_norm': 0.1481596678495407, 'learning_rate': 0.00014712292575006633, 'epoch': 0.73}\n",
      "{'loss': 0.0753, 'grad_norm': 0.31291964650154114, 'learning_rate': 0.00014600259408581687, 'epoch': 0.73}\n",
      "{'loss': 0.0419, 'grad_norm': 0.2423887699842453, 'learning_rate': 0.00014487488995678708, 'epoch': 0.74}\n",
      "{'loss': 0.0552, 'grad_norm': 0.14828327298164368, 'learning_rate': 0.00014373999409102362, 'epoch': 0.75}\n",
      "{'loss': 0.0404, 'grad_norm': 0.1671355962753296, 'learning_rate': 0.00014259808836913492, 'epoch': 0.76}\n",
      "{'loss': 0.0767, 'grad_norm': 0.35629069805145264, 'learning_rate': 0.00014144935579514246, 'epoch': 0.77}\n",
      "{'loss': 0.0449, 'grad_norm': 0.21464824676513672, 'learning_rate': 0.00014029398046715223, 'epoch': 0.77}\n",
      "{'loss': 0.0494, 'grad_norm': 0.2231532335281372, 'learning_rate': 0.00013913214754785095, 'epoch': 0.78}\n",
      "{'loss': 0.044, 'grad_norm': 0.16294214129447937, 'learning_rate': 0.00013796404323483132, 'epoch': 0.79}\n",
      "{'loss': 0.0504, 'grad_norm': 0.1532847136259079, 'learning_rate': 0.00013678985473075176, 'epoch': 0.8}\n",
      "{'loss': 0.0448, 'grad_norm': 0.1304529458284378, 'learning_rate': 0.00013560977021333497, 'epoch': 0.81}\n",
      "{'loss': 0.0653, 'grad_norm': 0.27236923575401306, 'learning_rate': 0.00013442397880521008, 'epoch': 0.81}\n",
      "{'loss': 0.0427, 'grad_norm': 0.2117595672607422, 'learning_rate': 0.0001332326705436037, 'epoch': 0.82}\n",
      "{'loss': 0.0596, 'grad_norm': 0.22144165635108948, 'learning_rate': 0.00013203603634988386, 'epoch': 0.83}\n",
      "{'loss': 0.0489, 'grad_norm': 0.173106849193573, 'learning_rate': 0.000130834267998963, 'epoch': 0.84}\n",
      "{'loss': 0.0532, 'grad_norm': 0.3698328733444214, 'learning_rate': 0.00012962755808856342, 'epoch': 0.84}\n",
      "{'loss': 0.0402, 'grad_norm': 0.17032617330551147, 'learning_rate': 0.00012841610000835125, 'epoch': 0.85}\n",
      "{'loss': 0.0558, 'grad_norm': 0.27817198634147644, 'learning_rate': 0.00012720008790894366, 'epoch': 0.86}\n",
      "{'loss': 0.0378, 'grad_norm': 0.21134985983371735, 'learning_rate': 0.00012597971667079361, 'epoch': 0.87}\n",
      "{'loss': 0.053, 'grad_norm': 0.6946887373924255, 'learning_rate': 0.0001247551818729582, 'epoch': 0.88}\n",
      "{'loss': 0.0359, 'grad_norm': 0.18278084695339203, 'learning_rate': 0.0001235266797617545, 'epoch': 0.88}\n",
      "{'loss': 0.051, 'grad_norm': 0.22802993655204773, 'learning_rate': 0.00012229440721930906, 'epoch': 0.89}\n",
      "{'loss': 0.0341, 'grad_norm': 0.2101507931947708, 'learning_rate': 0.00012105856173200498, 'epoch': 0.9}\n",
      "{'loss': 0.0545, 'grad_norm': 0.21445123851299286, 'learning_rate': 0.00011981934135883237, 'epoch': 0.91}\n",
      "{'loss': 0.0528, 'grad_norm': 0.21165607869625092, 'learning_rate': 0.00011857694469964715, 'epoch': 0.91}\n",
      "{'loss': 0.0461, 'grad_norm': 0.28440016508102417, 'learning_rate': 0.00011733157086334294, 'epoch': 0.92}\n",
      "{'loss': 0.0342, 'grad_norm': 0.0914030522108078, 'learning_rate': 0.0001160834194359416, 'epoch': 0.93}\n",
      "{'loss': 0.0705, 'grad_norm': 0.27181386947631836, 'learning_rate': 0.00011483269044860705, 'epoch': 0.94}\n",
      "{'loss': 0.0389, 'grad_norm': 0.1452869474887848, 'learning_rate': 0.000113579584345588, 'epoch': 0.95}\n",
      "{'loss': 0.0549, 'grad_norm': 0.23519569635391235, 'learning_rate': 0.0001123243019520942, 'epoch': 0.95}\n",
      "{'loss': 0.0369, 'grad_norm': 0.08609326183795929, 'learning_rate': 0.00011106704444211207, 'epoch': 0.96}\n",
      "{'loss': 0.0484, 'grad_norm': 0.2778990566730499, 'learning_rate': 0.000109808013306164, 'epoch': 0.97}\n",
      "{'loss': 0.038, 'grad_norm': 0.2629532814025879, 'learning_rate': 0.00010854741031901703, 'epoch': 0.98}\n",
      "{'loss': 0.0684, 'grad_norm': 0.2715413272380829, 'learning_rate': 0.00010728543750734622, 'epoch': 0.98}\n",
      "{'loss': 0.0352, 'grad_norm': 0.19750192761421204, 'learning_rate': 0.00010602229711735726, 'epoch': 0.99}\n",
      "{'loss': 0.0492, 'grad_norm': 0.19653365015983582, 'learning_rate': 0.00010475819158237425, 'epoch': 1.0}\n",
      "{'loss': 0.0291, 'grad_norm': 0.16222123801708221, 'learning_rate': 0.0001034933234903973, 'epoch': 1.01}\n",
      "{'loss': 0.047, 'grad_norm': 0.20708639919757843, 'learning_rate': 0.0001022278955516354, 'epoch': 1.02}\n",
      "{'loss': 0.0394, 'grad_norm': 0.1643044799566269, 'learning_rate': 0.00010096211056601958, 'epoch': 1.02}\n",
      "{'loss': 0.0458, 'grad_norm': 0.13272717595100403, 'learning_rate': 9.969617139070202e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0272, 'grad_norm': 0.18296131491661072, 'learning_rate': 9.84302809075455e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0453, 'grad_norm': 0.20088638365268707, 'learning_rate': 9.716464199060946e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0313, 'grad_norm': 0.14927376806735992, 'learning_rate': 9.589945747363667e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0467, 'grad_norm': 0.19226379692554474, 'learning_rate': 9.463493011754706e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0216, 'grad_norm': 0.14430613815784454, 'learning_rate': 9.337126257794255e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0389, 'grad_norm': 0.25263434648513794, 'learning_rate': 9.210865737262924e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0331, 'grad_norm': 0.198201984167099, 'learning_rate': 9.084731684916151e-05, 'epoch': 1.09}\n",
      "{'loss': 0.041, 'grad_norm': 0.026809802278876305, 'learning_rate': 8.958744315241341e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0349, 'grad_norm': 0.10379711538553238, 'learning_rate': 8.832923819218238e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0366, 'grad_norm': 0.09186951816082001, 'learning_rate': 8.707290361083107e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0299, 'grad_norm': 0.25562307238578796, 'learning_rate': 8.581864075097144e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0351, 'grad_norm': 0.05680067464709282, 'learning_rate': 8.456665062319742e-05, 'epoch': 1.13}\n",
      "{'loss': 0.038, 'grad_norm': 0.1409057229757309, 'learning_rate': 8.33171338738706e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0436, 'grad_norm': 0.11576171964406967, 'learning_rate': 8.207029075296392e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0322, 'grad_norm': 0.20560188591480255, 'learning_rate': 8.082632108196969e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0415, 'grad_norm': 0.13039858639240265, 'learning_rate': 7.958542422187538e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0295, 'grad_norm': 0.1456974297761917, 'learning_rate': 7.834779904121399e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0413, 'grad_norm': 0.17588438093662262, 'learning_rate': 7.711364388419278e-05, 'epoch': 1.17}\n",
      "{'loss': 0.028, 'grad_norm': 0.19113951921463013, 'learning_rate': 7.588315653890629e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0333, 'grad_norm': 0.15354913473129272, 'learning_rate': 7.465653420563845e-05, 'epoch': 1.19}\n",
      "{'loss': 0.025, 'grad_norm': 0.1852371096611023, 'learning_rate': 7.343397346525888e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0396, 'grad_norm': 0.12720173597335815, 'learning_rate': 7.221567024771849e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0254, 'grad_norm': 0.10735008865594864, 'learning_rate': 7.100181980064937e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0414, 'grad_norm': 0.19253745675086975, 'learning_rate': 6.979261665807389e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0276, 'grad_norm': 0.03230956196784973, 'learning_rate': 6.858825460922849e-05, 'epoch': 1.23}\n",
      "{'loss': 0.04, 'grad_norm': 0.23311646282672882, 'learning_rate': 6.738892666750651e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0197, 'grad_norm': 0.1685890108346939, 'learning_rate': 6.619482503952559e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0386, 'grad_norm': 0.0479617714881897, 'learning_rate': 6.500614109432419e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0302, 'grad_norm': 0.04361303523182869, 'learning_rate': 6.382306533269238e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0372, 'grad_norm': 0.1347581446170807, 'learning_rate': 6.264578735664194e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0241, 'grad_norm': 0.08843477815389633, 'learning_rate': 6.147449583902036e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0324, 'grad_norm': 0.1639636904001236, 'learning_rate': 6.030937849327356e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0236, 'grad_norm': 0.057000093162059784, 'learning_rate': 5.91506220433629e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0473, 'grad_norm': 0.10412025451660156, 'learning_rate': 5.79984121938401e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0266, 'grad_norm': 0.09578315913677216, 'learning_rate': 5.6852933600086125e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0379, 'grad_norm': 0.1010938212275505, 'learning_rate': 5.5714369838717874e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0352, 'grad_norm': 0.2028610110282898, 'learning_rate': 5.4582903378167716e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0463, 'grad_norm': 0.15233872830867767, 'learning_rate': 5.3458715549440984e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0329, 'grad_norm': 0.2081560641527176, 'learning_rate': 5.234198651705527e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0286, 'grad_norm': 0.09243500977754593, 'learning_rate': 5.12328952501671e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0305, 'grad_norm': 0.058443717658519745, 'learning_rate': 5.013161949388993e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0342, 'grad_norm': 0.07097695767879486, 'learning_rate': 4.903833574080825e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0226, 'grad_norm': 0.2500014901161194, 'learning_rate': 4.795321920269279e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0341, 'grad_norm': 0.1649225503206253, 'learning_rate': 4.687644378242044e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0256, 'grad_norm': 0.14892566204071045, 'learning_rate': 4.580818204610458e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0407, 'grad_norm': 0.09151675552129745, 'learning_rate': 4.4748605195438976e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0266, 'grad_norm': 0.07278190553188324, 'learning_rate': 4.36978830402608e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0422, 'grad_norm': 0.15508148074150085, 'learning_rate': 4.265618397133674e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0224, 'grad_norm': 0.0909937247633934, 'learning_rate': 4.162367493337601e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0326, 'grad_norm': 0.08767227828502655, 'learning_rate': 4.060052139827582e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0199, 'grad_norm': 0.1602378934621811, 'learning_rate': 3.958688733860237e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0262, 'grad_norm': 0.2317967414855957, 'learning_rate': 3.858293520131221e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0252, 'grad_norm': 0.023075230419635773, 'learning_rate': 3.758882588171837e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0279, 'grad_norm': 0.05247706174850464, 'learning_rate': 3.660471869770474e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0242, 'grad_norm': 0.1419275552034378, 'learning_rate': 3.563077136419373e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0332, 'grad_norm': 0.1773480474948883, 'learning_rate': 3.466713996787039e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0254, 'grad_norm': 0.18459045886993408, 'learning_rate': 3.371397894216766e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0304, 'grad_norm': 0.09970437735319138, 'learning_rate': 3.277144104251669e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0196, 'grad_norm': 0.22677859663963318, 'learning_rate': 3.183967732186582e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0358, 'grad_norm': 0.0915369838476181, 'learning_rate': 3.0918837106472686e-05, 'epoch': 1.5}\n",
      "{'loss': 0.021, 'grad_norm': 0.13343112170696259, 'learning_rate': 3.0009067971972716e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0396, 'grad_norm': 0.19207358360290527, 'learning_rate': 2.9110515719728594e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0238, 'grad_norm': 0.06222647801041603, 'learning_rate': 2.8223324353463644e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0365, 'grad_norm': 0.14597944915294647, 'learning_rate': 2.73476360561837e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0211, 'grad_norm': 0.05296514928340912, 'learning_rate': 2.6483591167390407e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0375, 'grad_norm': 0.03366486728191376, 'learning_rate': 2.5631328160590318e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0199, 'grad_norm': 0.15603026747703552, 'learning_rate': 2.479098362110267e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0203, 'grad_norm': 0.17754268646240234, 'learning_rate': 2.3962692224170114e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0224, 'grad_norm': 0.07745020091533661, 'learning_rate': 2.3146586713375395e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0389, 'grad_norm': 0.03380246460437775, 'learning_rate': 2.2342797879367418e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0265, 'grad_norm': 0.026049425825476646, 'learning_rate': 2.155145453890076e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0389, 'grad_norm': 0.05505313724279404, 'learning_rate': 2.0772683514191004e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0251, 'grad_norm': 0.10674291104078293, 'learning_rate': 2.0006609612590142e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0348, 'grad_norm': 0.25286248326301575, 'learning_rate': 1.9253355606584655e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0254, 'grad_norm': 0.06316901743412018, 'learning_rate': 1.851304221411967e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0318, 'grad_norm': 0.10090920329093933, 'learning_rate': 1.778578807925253e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0254, 'grad_norm': 0.14331893622875214, 'learning_rate': 1.707170975313879e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0285, 'grad_norm': 0.17967621982097626, 'learning_rate': 1.6370921675353223e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0194, 'grad_norm': 0.24462537467479706, 'learning_rate': 1.568353615554985e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0344, 'grad_norm': 0.10611876100301743, 'learning_rate': 1.5009663355462655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0179, 'grad_norm': 0.08899326622486115, 'learning_rate': 1.4349411271251134e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0208, 'grad_norm': 0.20975565910339355, 'learning_rate': 1.3702885716192348e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0205, 'grad_norm': 0.023059310391545296, 'learning_rate': 1.3070190303723352e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0255, 'grad_norm': 0.12308897078037262, 'learning_rate': 1.2451426430835733e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0198, 'grad_norm': 0.2202647179365158, 'learning_rate': 1.1846693261825525e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0352, 'grad_norm': 0.20158152282238007, 'learning_rate': 1.1256087712401087e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0213, 'grad_norm': 0.0767163634300232, 'learning_rate': 1.0679704434151016e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0308, 'grad_norm': 0.22335465252399445, 'learning_rate': 1.0117635799375291e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0193, 'grad_norm': 0.17198410630226135, 'learning_rate': 9.569971886281392e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0267, 'grad_norm': 0.17353568971157074, 'learning_rate': 9.036800464548157e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0243, 'grad_norm': 0.08238823711872101, 'learning_rate': 8.51820698125979e-06, 'epoch': 1.74}\n",
      "{'loss': 0.028, 'grad_norm': 0.23704172670841217, 'learning_rate': 8.014274547211808e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0243, 'grad_norm': 0.12837642431259155, 'learning_rate': 7.525083923591592e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0249, 'grad_norm': 0.03588886186480522, 'learning_rate': 7.050713509035478e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0208, 'grad_norm': 0.14019536972045898, 'learning_rate': 6.591239327064391e-06, 'epoch': 1.77}\n",
      "{'loss': 0.031, 'grad_norm': 0.02937931939959526, 'learning_rate': 6.146735013900173e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0261, 'grad_norm': 0.12394514679908752, 'learning_rate': 5.717271806664559e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0246, 'grad_norm': 0.031466539949178696, 'learning_rate': 5.302918531962464e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0139, 'grad_norm': 0.027209661900997162, 'learning_rate': 4.903741594851841e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0307, 'grad_norm': 0.18120719492435455, 'learning_rate': 4.519804968201313e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0202, 'grad_norm': 0.13488014042377472, 'learning_rate': 4.151170182437924e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0299, 'grad_norm': 0.028063567355275154, 'learning_rate': 3.7978963156860446e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0183, 'grad_norm': 0.0646483302116394, 'learning_rate': 3.4600399842994237e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0256, 'grad_norm': 0.10572118312120438, 'learning_rate': 3.13765533378777e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0167, 'grad_norm': 0.18584172427654266, 'learning_rate': 2.8307940301392164e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0322, 'grad_norm': 0.028159258887171745, 'learning_rate': 2.539505251540353e-06, 'epoch': 1.86}\n",
      "{'loss': 0.02, 'grad_norm': 0.16383618116378784, 'learning_rate': 2.263835680494686e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0325, 'grad_norm': 0.1757473349571228, 'learning_rate': 2.003829496341325e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0199, 'grad_norm': 0.037028055638074875, 'learning_rate': 1.7595283681746678e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0258, 'grad_norm': 0.1590198278427124, 'learning_rate': 1.5309714481664183e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0191, 'grad_norm': 0.04241291433572769, 'learning_rate': 1.3181953652910305e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0203, 'grad_norm': 0.13229146599769592, 'learning_rate': 1.121234219455447e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0205, 'grad_norm': 0.15318424999713898, 'learning_rate': 9.401195760341708e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0242, 'grad_norm': 0.22884990274906158, 'learning_rate': 7.748804608105675e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0236, 'grad_norm': 0.03139727562665939, 'learning_rate': 6.255433553250978e-07, 'epoch': 1.93}\n",
      "{'loss': 0.022, 'grad_norm': 0.13683681190013885, 'learning_rate': 4.921321926313893e-07, 'epoch': 1.94}\n",
      "{'loss': 0.0128, 'grad_norm': 0.13990086317062378, 'learning_rate': 3.746683534606277e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0302, 'grad_norm': 0.24335864186286926, 'learning_rate': 2.7317066279506363e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0229, 'grad_norm': 0.200803741812706, 'learning_rate': 1.8765538685108218e-07, 'epoch': 1.96}\n",
      "{'loss': 0.0295, 'grad_norm': 0.02536185458302498, 'learning_rate': 1.1813623047236544e-07, 'epoch': 1.97}\n",
      "{'loss': 0.019, 'grad_norm': 0.1315106302499771, 'learning_rate': 6.462433493347187e-08, 'epoch': 1.98}\n",
      "{'loss': 0.0301, 'grad_norm': 0.05901605635881424, 'learning_rate': 2.712827615437563e-08, 'epoch': 1.99}\n",
      "{'loss': 0.0253, 'grad_norm': 0.11585033684968948, 'learning_rate': 5.654063326032688e-09, 'epoch': 1.99}\n",
      "{'train_runtime': 30004.4917, 'train_samples_per_second': 0.64, 'train_steps_per_second': 0.213, 'train_loss': 0.0634373522703166, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6396, training_loss=0.0634373522703166, metrics={'train_runtime': 30004.4917, 'train_samples_per_second': 0.64, 'train_steps_per_second': 0.213, 'total_flos': 8.049339626815488e+17, 'train_loss': 0.0634373522703166, 'epoch': 1.9997915581031789})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Mistral-7B-Instruct-v0.3-luis_tokenizer/tokenizer_config.json',\n",
       " 'Mistral-7B-Instruct-v0.3-luis_tokenizer/special_tokens_map.json',\n",
       " 'Mistral-7B-Instruct-v0.3-luis_tokenizer/tokenizer.model',\n",
       " 'Mistral-7B-Instruct-v0.3-luis_tokenizer/added_tokens.json',\n",
       " 'Mistral-7B-Instruct-v0.3-luis_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model + \"_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
