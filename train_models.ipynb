{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TIGER-Lab/StructLM-7B\"\n",
    "\n",
    "dataset_name = \"kokujin/prompts_1\"\n",
    "\n",
    "new_model = \"StructLM-7B-luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{model_name}/\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 3\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 1100\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd94557506f4aeb8424724b1669c115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2aae9d931347baa1252b0ef0a010b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.423, 'grad_norm': 0.14594127237796783, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.3581, 'grad_norm': 0.19861209392547607, 'learning_rate': 5.208333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 0.9733, 'grad_norm': 0.1769675761461258, 'learning_rate': 7.8125e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4944, 'grad_norm': 0.602828323841095, 'learning_rate': 0.00010416666666666667, 'epoch': 0.03}\n",
      "{'loss': 0.3355, 'grad_norm': 0.14405296742916107, 'learning_rate': 0.00013020833333333333, 'epoch': 0.04}\n",
      "{'loss': 0.3074, 'grad_norm': 0.3482108414173126, 'learning_rate': 0.00015625, 'epoch': 0.05}\n",
      "{'loss': 0.2652, 'grad_norm': 0.13975800573825836, 'learning_rate': 0.00018229166666666667, 'epoch': 0.05}\n",
      "{'loss': 0.2844, 'grad_norm': 0.18033498525619507, 'learning_rate': 0.00019999917944905216, 'epoch': 0.06}\n",
      "{'loss': 0.2417, 'grad_norm': 0.1609336882829666, 'learning_rate': 0.00019998603811858571, 'epoch': 0.07}\n",
      "{'loss': 0.2604, 'grad_norm': 0.19641198217868805, 'learning_rate': 0.00019995687283209658, 'epoch': 0.08}\n",
      "{'loss': 0.2171, 'grad_norm': 0.13977476954460144, 'learning_rate': 0.00019991168826367011, 'epoch': 0.09}\n",
      "{'loss': 0.235, 'grad_norm': 0.18794649839401245, 'learning_rate': 0.00019985049165467257, 'epoch': 0.09}\n",
      "{'loss': 0.2178, 'grad_norm': 0.16783398389816284, 'learning_rate': 0.00019977329281259108, 'epoch': 0.1}\n",
      "{'loss': 0.2348, 'grad_norm': 0.18690411746501923, 'learning_rate': 0.00019968010410946154, 'epoch': 0.11}\n",
      "{'loss': 0.1994, 'grad_norm': 0.11034905910491943, 'learning_rate': 0.00019957094047988582, 'epoch': 0.12}\n",
      "{'loss': 0.2166, 'grad_norm': 0.16415157914161682, 'learning_rate': 0.00019944581941863857, 'epoch': 0.13}\n",
      "{'loss': 0.1838, 'grad_norm': 0.11332754790782928, 'learning_rate': 0.00019930476097786327, 'epoch': 0.13}\n",
      "{'loss': 0.2274, 'grad_norm': 0.2497699111700058, 'learning_rate': 0.0001991477877638587, 'epoch': 0.14}\n",
      "{'loss': 0.194, 'grad_norm': 0.11457492411136627, 'learning_rate': 0.00019897492493345598, 'epoch': 0.15}\n",
      "{'loss': 0.2042, 'grad_norm': 0.11682303994894028, 'learning_rate': 0.00019878620018998696, 'epoch': 0.16}\n",
      "{'loss': 0.178, 'grad_norm': 0.14507901668548584, 'learning_rate': 0.00019858164377884436, 'epoch': 0.16}\n",
      "{'loss': 0.2004, 'grad_norm': 0.13055486977100372, 'learning_rate': 0.00019836128848263465, 'epoch': 0.17}\n",
      "{'loss': 0.1848, 'grad_norm': 0.11945588886737823, 'learning_rate': 0.0001981251696159241, 'epoch': 0.18}\n",
      "{'loss': 0.1953, 'grad_norm': 0.15185748040676117, 'learning_rate': 0.00019787332501957941, 'epoch': 0.19}\n",
      "{'loss': 0.1693, 'grad_norm': 0.12302836030721664, 'learning_rate': 0.0001976057950547031, 'epoch': 0.2}\n",
      "{'loss': 0.2142, 'grad_norm': 0.11295034736394882, 'learning_rate': 0.00019732262259616523, 'epoch': 0.2}\n",
      "{'loss': 0.1579, 'grad_norm': 0.1161443367600441, 'learning_rate': 0.0001970238530257322, 'epoch': 0.21}\n",
      "{'loss': 0.1841, 'grad_norm': 0.09841138124465942, 'learning_rate': 0.00019670953422479368, 'epoch': 0.22}\n",
      "{'loss': 0.1512, 'grad_norm': 0.14099529385566711, 'learning_rate': 0.00019637971656668918, 'epoch': 0.23}\n",
      "{'loss': 0.1759, 'grad_norm': 0.2477014660835266, 'learning_rate': 0.0001960344529086351, 'epoch': 0.23}\n",
      "{'loss': 0.1653, 'grad_norm': 0.12504464387893677, 'learning_rate': 0.00019567379858325356, 'epoch': 0.24}\n",
      "{'loss': 0.1738, 'grad_norm': 0.3727221190929413, 'learning_rate': 0.000195297811389705, 'epoch': 0.25}\n",
      "{'loss': 0.1564, 'grad_norm': 0.12596112489700317, 'learning_rate': 0.00019490655158442484, 'epoch': 0.26}\n",
      "{'loss': 0.1763, 'grad_norm': 0.1295066773891449, 'learning_rate': 0.00019450008187146684, 'epoch': 0.27}\n",
      "{'loss': 0.1543, 'grad_norm': 0.1178160235285759, 'learning_rate': 0.00019407846739245415, 'epoch': 0.27}\n",
      "{'loss': 0.1919, 'grad_norm': 0.12039966136217117, 'learning_rate': 0.00019364177571613926, 'epoch': 0.28}\n",
      "{'loss': 0.1404, 'grad_norm': 0.11229131370782852, 'learning_rate': 0.00019319007682757556, 'epoch': 0.29}\n",
      "{'loss': 0.1682, 'grad_norm': 0.12833473086357117, 'learning_rate': 0.0001927234431169014, 'epoch': 0.3}\n",
      "{'loss': 0.1364, 'grad_norm': 0.13638420403003693, 'learning_rate': 0.00019224194936773853, 'epoch': 0.3}\n",
      "{'loss': 0.1482, 'grad_norm': 0.182381272315979, 'learning_rate': 0.00019174567274520728, 'epoch': 0.31}\n",
      "{'loss': 0.134, 'grad_norm': 0.13000649213790894, 'learning_rate': 0.00019123469278355985, 'epoch': 0.32}\n",
      "{'loss': 0.1496, 'grad_norm': 0.1716369390487671, 'learning_rate': 0.00019070909137343408, 'epoch': 0.33}\n",
      "{'loss': 0.1236, 'grad_norm': 0.12828557193279266, 'learning_rate': 0.0001901689527487294, 'epoch': 0.34}\n",
      "{'loss': 0.1437, 'grad_norm': 0.13589757680892944, 'learning_rate': 0.0001896143634731074, 'epoch': 0.34}\n",
      "{'loss': 0.1415, 'grad_norm': 0.11323893815279007, 'learning_rate': 0.00018904541242611902, 'epoch': 0.35}\n",
      "{'loss': 0.155, 'grad_norm': 0.19907104969024658, 'learning_rate': 0.00018846219078896037, 'epoch': 0.36}\n",
      "{'loss': 0.1183, 'grad_norm': 0.11822272837162018, 'learning_rate': 0.00018786479202986006, 'epoch': 0.37}\n",
      "{'loss': 0.1598, 'grad_norm': 0.1565147489309311, 'learning_rate': 0.0001872533118890997, 'epoch': 0.38}\n",
      "{'loss': 0.1297, 'grad_norm': 0.13146431744098663, 'learning_rate': 0.00018662784836367028, 'epoch': 0.38}\n",
      "{'loss': 0.1463, 'grad_norm': 0.143926203250885, 'learning_rate': 0.00018598850169156722, 'epoch': 0.39}\n",
      "{'loss': 0.1321, 'grad_norm': 0.11255403608083725, 'learning_rate': 0.00018533537433572581, 'epoch': 0.4}\n",
      "{'loss': 0.1368, 'grad_norm': 0.16874153912067413, 'learning_rate': 0.00018466857096760046, 'epoch': 0.41}\n",
      "{'loss': 0.1179, 'grad_norm': 0.11725971847772598, 'learning_rate': 0.00018398819845038972, 'epoch': 0.41}\n",
      "{'loss': 0.1518, 'grad_norm': 0.26376208662986755, 'learning_rate': 0.0001832943658219103, 'epoch': 0.42}\n",
      "{'loss': 0.1251, 'grad_norm': 0.14790816605091095, 'learning_rate': 0.00018258718427712238, 'epoch': 0.43}\n",
      "{'loss': 0.141, 'grad_norm': 0.1723167896270752, 'learning_rate': 0.00018186676715030924, 'epoch': 0.44}\n",
      "{'loss': 0.1208, 'grad_norm': 0.13394208252429962, 'learning_rate': 0.0001811332298969142, 'epoch': 0.45}\n",
      "{'loss': 0.1524, 'grad_norm': 0.19469794631004333, 'learning_rate': 0.0001803866900750376, 'epoch': 0.45}\n",
      "{'loss': 0.1113, 'grad_norm': 0.13278089463710785, 'learning_rate': 0.0001796272673265963, 'epoch': 0.46}\n",
      "{'loss': 0.1263, 'grad_norm': 0.13124729692935944, 'learning_rate': 0.00017885508335815014, 'epoch': 0.47}\n",
      "{'loss': 0.1212, 'grad_norm': 0.13141116499900818, 'learning_rate': 0.0001780702619213967, 'epoch': 0.48}\n",
      "{'loss': 0.12, 'grad_norm': 0.17526577413082123, 'learning_rate': 0.0001772729287933387, 'epoch': 0.48}\n",
      "{'loss': 0.1043, 'grad_norm': 0.11748629063367844, 'learning_rate': 0.00017646321175612668, 'epoch': 0.49}\n",
      "{'loss': 0.1465, 'grad_norm': 0.1497262865304947, 'learning_rate': 0.00017564124057658056, 'epoch': 0.5}\n",
      "{'loss': 0.0966, 'grad_norm': 0.13054420053958893, 'learning_rate': 0.00017480714698539266, 'epoch': 0.51}\n",
      "{'loss': 0.1255, 'grad_norm': 0.21636267006397247, 'learning_rate': 0.00017396106465601663, 'epoch': 0.52}\n",
      "{'loss': 0.1015, 'grad_norm': 0.13989101350307465, 'learning_rate': 0.0001731031291832444, 'epoch': 0.52}\n",
      "{'loss': 0.1312, 'grad_norm': 0.12669432163238525, 'learning_rate': 0.0001722334780614756, 'epoch': 0.53}\n",
      "{'loss': 0.1025, 'grad_norm': 0.1306009292602539, 'learning_rate': 0.00017135225066268255, 'epoch': 0.54}\n",
      "{'loss': 0.1419, 'grad_norm': 0.2528972327709198, 'learning_rate': 0.00017045958821407405, 'epoch': 0.55}\n",
      "{'loss': 0.1098, 'grad_norm': 0.12520478665828705, 'learning_rate': 0.00016955563377546207, 'epoch': 0.55}\n",
      "{'loss': 0.1269, 'grad_norm': 0.2129993736743927, 'learning_rate': 0.0001686405322163349, 'epoch': 0.56}\n",
      "{'loss': 0.1065, 'grad_norm': 0.12355776131153107, 'learning_rate': 0.00016771443019263983, 'epoch': 0.57}\n",
      "{'loss': 0.1204, 'grad_norm': 0.16234388947486877, 'learning_rate': 0.00016677747612327997, 'epoch': 0.58}\n",
      "{'loss': 0.093, 'grad_norm': 0.11190716922283173, 'learning_rate': 0.00016582982016632818, 'epoch': 0.59}\n",
      "{'loss': 0.1153, 'grad_norm': 0.12065290659666061, 'learning_rate': 0.00016487161419496263, 'epoch': 0.59}\n",
      "{'loss': 0.0928, 'grad_norm': 0.1138220727443695, 'learning_rate': 0.00016390301177312722, 'epoch': 0.6}\n",
      "{'loss': 0.1256, 'grad_norm': 0.16219855844974518, 'learning_rate': 0.00016292416813092105, 'epoch': 0.61}\n",
      "{'loss': 0.1035, 'grad_norm': 0.15289220213890076, 'learning_rate': 0.00016193524013972114, 'epoch': 0.62}\n",
      "{'loss': 0.1217, 'grad_norm': 0.2328803688287735, 'learning_rate': 0.00016093638628704167, 'epoch': 0.63}\n",
      "{'loss': 0.0928, 'grad_norm': 0.1326129287481308, 'learning_rate': 0.0001599277666511347, 'epoch': 0.63}\n",
      "{'loss': 0.1145, 'grad_norm': 0.19925303757190704, 'learning_rate': 0.00015890954287533555, 'epoch': 0.64}\n",
      "{'loss': 0.0903, 'grad_norm': 0.10995218902826309, 'learning_rate': 0.00015788187814215764, 'epoch': 0.65}\n",
      "{'loss': 0.1183, 'grad_norm': 0.14275571703910828, 'learning_rate': 0.00015684493714714047, 'epoch': 0.66}\n",
      "{'loss': 0.0833, 'grad_norm': 0.12511374056339264, 'learning_rate': 0.00015579888607245517, 'epoch': 0.66}\n",
      "{'loss': 0.1228, 'grad_norm': 0.17842432856559753, 'learning_rate': 0.000154743892560272, 'epoch': 0.67}\n",
      "{'loss': 0.0826, 'grad_norm': 0.09889087826013565, 'learning_rate': 0.00015368012568589342, 'epoch': 0.68}\n",
      "{'loss': 0.1186, 'grad_norm': 0.16230441629886627, 'learning_rate': 0.00015260775593065802, 'epoch': 0.69}\n",
      "{'loss': 0.0776, 'grad_norm': 0.11392246931791306, 'learning_rate': 0.00015152695515461865, 'epoch': 0.7}\n",
      "{'loss': 0.1148, 'grad_norm': 0.1406807005405426, 'learning_rate': 0.00015043789656899988, 'epoch': 0.7}\n",
      "{'loss': 0.0773, 'grad_norm': 0.14356863498687744, 'learning_rate': 0.00014934075470843887, 'epoch': 0.71}\n",
      "{'loss': 0.1118, 'grad_norm': 0.15840110182762146, 'learning_rate': 0.00014823570540301408, 'epoch': 0.72}\n",
      "{'loss': 0.0898, 'grad_norm': 0.10787177085876465, 'learning_rate': 0.00014712292575006633, 'epoch': 0.73}\n",
      "{'loss': 0.1248, 'grad_norm': 0.2164173275232315, 'learning_rate': 0.00014600259408581687, 'epoch': 0.73}\n",
      "{'loss': 0.0708, 'grad_norm': 0.0803961306810379, 'learning_rate': 0.00014487488995678708, 'epoch': 0.74}\n",
      "{'loss': 0.1041, 'grad_norm': 0.16202154755592346, 'learning_rate': 0.00014373999409102362, 'epoch': 0.75}\n",
      "{'loss': 0.073, 'grad_norm': 0.12513405084609985, 'learning_rate': 0.00014259808836913492, 'epoch': 0.76}\n",
      "{'loss': 0.119, 'grad_norm': 0.21335402131080627, 'learning_rate': 0.00014144935579514246, 'epoch': 0.77}\n",
      "{'loss': 0.073, 'grad_norm': 0.13123224675655365, 'learning_rate': 0.00014029398046715223, 'epoch': 0.77}\n",
      "{'loss': 0.0936, 'grad_norm': 0.13963523507118225, 'learning_rate': 0.00013913214754785095, 'epoch': 0.78}\n",
      "{'loss': 0.0827, 'grad_norm': 0.12507574260234833, 'learning_rate': 0.00013796404323483132, 'epoch': 0.79}\n",
      "{'loss': 0.087, 'grad_norm': 0.12456454336643219, 'learning_rate': 0.00013678985473075176, 'epoch': 0.8}\n",
      "{'loss': 0.0799, 'grad_norm': 0.12224282324314117, 'learning_rate': 0.00013560977021333497, 'epoch': 0.81}\n",
      "{'loss': 0.1119, 'grad_norm': 0.16335850954055786, 'learning_rate': 0.00013442397880521008, 'epoch': 0.81}\n",
      "{'loss': 0.0807, 'grad_norm': 0.12003367394208908, 'learning_rate': 0.0001332326705436037, 'epoch': 0.82}\n",
      "{'loss': 0.1018, 'grad_norm': 0.17386946082115173, 'learning_rate': 0.00013203603634988386, 'epoch': 0.83}\n",
      "{'loss': 0.0751, 'grad_norm': 0.1349707692861557, 'learning_rate': 0.000130834267998963, 'epoch': 0.84}\n",
      "{'loss': 0.1086, 'grad_norm': 0.18134523928165436, 'learning_rate': 0.00012962755808856342, 'epoch': 0.84}\n",
      "{'loss': 0.0803, 'grad_norm': 0.129630446434021, 'learning_rate': 0.00012841610000835125, 'epoch': 0.85}\n",
      "{'loss': 0.1004, 'grad_norm': 0.17309688031673431, 'learning_rate': 0.00012720008790894366, 'epoch': 0.86}\n",
      "{'loss': 0.0777, 'grad_norm': 0.10843831300735474, 'learning_rate': 0.00012597971667079361, 'epoch': 0.87}\n",
      "{'loss': 0.099, 'grad_norm': 0.14617060124874115, 'learning_rate': 0.0001247551818729582, 'epoch': 0.88}\n",
      "{'loss': 0.0701, 'grad_norm': 0.11390377581119537, 'learning_rate': 0.0001235266797617545, 'epoch': 0.88}\n",
      "{'loss': 0.0852, 'grad_norm': 0.14408794045448303, 'learning_rate': 0.00012229440721930906, 'epoch': 0.89}\n",
      "{'loss': 0.0583, 'grad_norm': 0.09277956187725067, 'learning_rate': 0.00012105856173200498, 'epoch': 0.9}\n",
      "{'loss': 0.0981, 'grad_norm': 0.14752984046936035, 'learning_rate': 0.00011981934135883237, 'epoch': 0.91}\n",
      "{'loss': 0.0906, 'grad_norm': 0.14049841463565826, 'learning_rate': 0.00011857694469964715, 'epoch': 0.91}\n",
      "{'loss': 0.0896, 'grad_norm': 0.16831791400909424, 'learning_rate': 0.00011733157086334294, 'epoch': 0.92}\n",
      "{'loss': 0.0684, 'grad_norm': 0.15764081478118896, 'learning_rate': 0.0001160834194359416, 'epoch': 0.93}\n",
      "{'loss': 0.1113, 'grad_norm': 0.17426642775535583, 'learning_rate': 0.00011483269044860705, 'epoch': 0.94}\n",
      "{'loss': 0.0774, 'grad_norm': 0.10094071924686432, 'learning_rate': 0.000113579584345588, 'epoch': 0.95}\n",
      "{'loss': 0.0945, 'grad_norm': 0.16070006787776947, 'learning_rate': 0.0001123243019520942, 'epoch': 0.95}\n",
      "{'loss': 0.0674, 'grad_norm': 0.18154111504554749, 'learning_rate': 0.00011106704444211207, 'epoch': 0.96}\n",
      "{'loss': 0.0879, 'grad_norm': 0.156692236661911, 'learning_rate': 0.000109808013306164, 'epoch': 0.97}\n",
      "{'loss': 0.0709, 'grad_norm': 0.14553603529930115, 'learning_rate': 0.00010854741031901703, 'epoch': 0.98}\n",
      "{'loss': 0.1098, 'grad_norm': 0.17945434153079987, 'learning_rate': 0.00010728543750734622, 'epoch': 0.98}\n",
      "{'loss': 0.0707, 'grad_norm': 0.17188279330730438, 'learning_rate': 0.00010602229711735726, 'epoch': 0.99}\n",
      "{'loss': 0.0873, 'grad_norm': 0.1285799741744995, 'learning_rate': 0.00010475819158237425, 'epoch': 1.0}\n",
      "{'loss': 0.0596, 'grad_norm': 0.11610758304595947, 'learning_rate': 0.0001034933234903973, 'epoch': 1.01}\n",
      "{'loss': 0.0921, 'grad_norm': 0.15578778088092804, 'learning_rate': 0.0001022278955516354, 'epoch': 1.02}\n",
      "{'loss': 0.0653, 'grad_norm': 0.10335413366556168, 'learning_rate': 0.00010096211056601958, 'epoch': 1.02}\n",
      "{'loss': 0.0975, 'grad_norm': 0.1463235765695572, 'learning_rate': 9.969617139070202e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0611, 'grad_norm': 0.0791129469871521, 'learning_rate': 9.84302809075455e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0838, 'grad_norm': 0.16031472384929657, 'learning_rate': 9.716464199060946e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0674, 'grad_norm': 0.10740784555673599, 'learning_rate': 9.589945747363667e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0899, 'grad_norm': 0.12593664228916168, 'learning_rate': 9.463493011754706e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0435, 'grad_norm': 0.1360887885093689, 'learning_rate': 9.337126257794255e-05, 'epoch': 1.07}\n",
      "{'loss': 0.068, 'grad_norm': 0.17487294971942902, 'learning_rate': 9.210865737262924e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0632, 'grad_norm': 0.09997982531785965, 'learning_rate': 9.084731684916151e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0868, 'grad_norm': 0.08060628175735474, 'learning_rate': 8.958744315241341e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0651, 'grad_norm': 0.10105565935373306, 'learning_rate': 8.832923819218238e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0765, 'grad_norm': 0.14751268923282623, 'learning_rate': 8.707290361083107e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0557, 'grad_norm': 0.1443415880203247, 'learning_rate': 8.581864075097144e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0648, 'grad_norm': 0.14357227087020874, 'learning_rate': 8.456665062319742e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0627, 'grad_norm': 0.1056608185172081, 'learning_rate': 8.33171338738706e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0894, 'grad_norm': 0.138380229473114, 'learning_rate': 8.207029075296392e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0589, 'grad_norm': 0.08892809599637985, 'learning_rate': 8.082632108196969e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0757, 'grad_norm': 0.08757421374320984, 'learning_rate': 7.958542422187538e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0589, 'grad_norm': 0.13425061106681824, 'learning_rate': 7.834779904121399e-05, 'epoch': 1.16}\n",
      "{'loss': 0.076, 'grad_norm': 0.1496785432100296, 'learning_rate': 7.711364388419278e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0578, 'grad_norm': 0.12270303815603256, 'learning_rate': 7.588315653890629e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0634, 'grad_norm': 0.11311302334070206, 'learning_rate': 7.465653420563845e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0558, 'grad_norm': 0.13983571529388428, 'learning_rate': 7.343397346525888e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0699, 'grad_norm': 0.13892263174057007, 'learning_rate': 7.221567024771849e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0471, 'grad_norm': 0.11069196462631226, 'learning_rate': 7.100181980064937e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0794, 'grad_norm': 0.11397205293178558, 'learning_rate': 6.979261665807389e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0548, 'grad_norm': 0.08246912062168121, 'learning_rate': 6.858825460922849e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0708, 'grad_norm': 0.1177457794547081, 'learning_rate': 6.738892666750651e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0384, 'grad_norm': 0.07440993189811707, 'learning_rate': 6.619482503952559e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0727, 'grad_norm': 0.14986029267311096, 'learning_rate': 6.500614109432419e-05, 'epoch': 1.25}\n",
      "{'loss': 0.057, 'grad_norm': 0.11996316909790039, 'learning_rate': 6.382306533269238e-05, 'epoch': 1.26}\n",
      "{'loss': 0.072, 'grad_norm': 0.12583588063716888, 'learning_rate': 6.264578735664194e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0544, 'grad_norm': 0.20531374216079712, 'learning_rate': 6.147449583902036e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0564, 'grad_norm': 0.13313372433185577, 'learning_rate': 6.030937849327356e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0426, 'grad_norm': 0.15436576306819916, 'learning_rate': 5.91506220433629e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0817, 'grad_norm': 0.1448550671339035, 'learning_rate': 5.79984121938401e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0416, 'grad_norm': 0.10370153933763504, 'learning_rate': 5.6852933600086125e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0753, 'grad_norm': 0.12043353170156479, 'learning_rate': 5.5714369838717874e-05, 'epoch': 1.31}\n",
      "{'loss': 0.053, 'grad_norm': 0.1162615641951561, 'learning_rate': 5.4582903378167716e-05, 'epoch': 1.32}\n",
      "{'loss': 0.093, 'grad_norm': 0.1076437309384346, 'learning_rate': 5.3458715549440984e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0552, 'grad_norm': 0.0468624122440815, 'learning_rate': 5.234198651705527e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0707, 'grad_norm': 0.17811940610408783, 'learning_rate': 5.12328952501671e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0478, 'grad_norm': 0.11843479424715042, 'learning_rate': 5.013161949388993e-05, 'epoch': 1.35}\n",
      "{'loss': 0.073, 'grad_norm': 0.14972515404224396, 'learning_rate': 4.903833574080825e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0504, 'grad_norm': 0.05959334224462509, 'learning_rate': 4.795321920269279e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0532, 'grad_norm': 0.11737193167209625, 'learning_rate': 4.687644378242044e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0488, 'grad_norm': 0.10131950676441193, 'learning_rate': 4.580818204610458e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0742, 'grad_norm': 0.12758570909500122, 'learning_rate': 4.4748605195438976e-05, 'epoch': 1.39}\n",
      "{'loss': 0.055, 'grad_norm': 0.09582394361495972, 'learning_rate': 4.36978830402608e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0738, 'grad_norm': 0.13481608033180237, 'learning_rate': 4.265618397133674e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0433, 'grad_norm': 0.05680245906114578, 'learning_rate': 4.162367493337601e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0715, 'grad_norm': 0.1570988893508911, 'learning_rate': 4.060052139827582e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0405, 'grad_norm': 0.09380832314491272, 'learning_rate': 3.958688733860237e-05, 'epoch': 1.43}\n",
      "{'loss': 0.056, 'grad_norm': 0.16131095588207245, 'learning_rate': 3.858293520131221e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0476, 'grad_norm': 0.06745059788227081, 'learning_rate': 3.758882588171837e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0599, 'grad_norm': 0.1174284964799881, 'learning_rate': 3.660471869770474e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0528, 'grad_norm': 0.09241286665201187, 'learning_rate': 3.563077136419373e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0678, 'grad_norm': 0.1354350745677948, 'learning_rate': 3.466713996787039e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0609, 'grad_norm': 0.11426115781068802, 'learning_rate': 3.371397894216766e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0582, 'grad_norm': 0.11279243230819702, 'learning_rate': 3.277144104251669e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0368, 'grad_norm': 0.12732049822807312, 'learning_rate': 3.183967732186582e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0724, 'grad_norm': 0.10426943004131317, 'learning_rate': 3.0918837106472686e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0445, 'grad_norm': 0.1450219303369522, 'learning_rate': 3.0009067971972716e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0737, 'grad_norm': 0.12447300553321838, 'learning_rate': 2.9110515719728594e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0431, 'grad_norm': 0.10895777493715286, 'learning_rate': 2.8223324353463644e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0721, 'grad_norm': 0.09013832360506058, 'learning_rate': 2.73476360561837e-05, 'epoch': 1.53}\n",
      "{'loss': 0.029, 'grad_norm': 0.07389712333679199, 'learning_rate': 2.6483591167390407e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0803, 'grad_norm': 0.10212524980306625, 'learning_rate': 2.5631328160590318e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0377, 'grad_norm': 0.08793440461158752, 'learning_rate': 2.479098362110267e-05, 'epoch': 1.56}\n",
      "{'loss': 0.044, 'grad_norm': 0.13979513943195343, 'learning_rate': 2.3962692224170114e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0436, 'grad_norm': 0.06280594319105148, 'learning_rate': 2.3146586713375395e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0721, 'grad_norm': 0.08560126274824142, 'learning_rate': 2.2342797879367418e-05, 'epoch': 1.58}\n",
      "{'loss': 0.065, 'grad_norm': 0.15116173028945923, 'learning_rate': 2.155145453890076e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0728, 'grad_norm': 0.1250026822090149, 'learning_rate': 2.0772683514191004e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0486, 'grad_norm': 0.042572472244501114, 'learning_rate': 2.0006609612590142e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0669, 'grad_norm': 0.1309526115655899, 'learning_rate': 1.9253355606584655e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0458, 'grad_norm': 0.12829063832759857, 'learning_rate': 1.851304221411967e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0731, 'grad_norm': 0.0859890878200531, 'learning_rate': 1.778578807925253e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0416, 'grad_norm': 0.0731671154499054, 'learning_rate': 1.707170975313879e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0706, 'grad_norm': 0.1356511265039444, 'learning_rate': 1.6370921675353223e-05, 'epoch': 1.64}\n",
      "{'loss': 0.041, 'grad_norm': 0.035815607756376266, 'learning_rate': 1.568353615554985e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0628, 'grad_norm': 0.13565154373645782, 'learning_rate': 1.5009663355462655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0394, 'grad_norm': 0.04281824454665184, 'learning_rate': 1.4349411271251134e-05, 'epoch': 1.66}\n",
      "{'loss': 0.046, 'grad_norm': 0.12670378386974335, 'learning_rate': 1.3702885716192348e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0362, 'grad_norm': 0.12649276852607727, 'learning_rate': 1.3070190303723352e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0658, 'grad_norm': 0.11440331488847733, 'learning_rate': 1.2451426430835733e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0436, 'grad_norm': 0.1305019110441208, 'learning_rate': 1.1846693261825525e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0737, 'grad_norm': 0.1532101333141327, 'learning_rate': 1.1256087712401087e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0425, 'grad_norm': 0.07082261145114899, 'learning_rate': 1.0679704434151016e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0626, 'grad_norm': 0.1538669317960739, 'learning_rate': 1.0117635799375291e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0429, 'grad_norm': 0.1538805067539215, 'learning_rate': 9.569971886281392e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0561, 'grad_norm': 0.16357557475566864, 'learning_rate': 9.036800464548157e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0363, 'grad_norm': 0.08409008383750916, 'learning_rate': 8.51820698125979e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0636, 'grad_norm': 0.1327177733182907, 'learning_rate': 8.014274547211808e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0452, 'grad_norm': 0.026232730597257614, 'learning_rate': 7.525083923591592e-06, 'epoch': 1.76}\n",
      "{'loss': 0.063, 'grad_norm': 0.13959196209907532, 'learning_rate': 7.050713509035478e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0445, 'grad_norm': 0.1190897673368454, 'learning_rate': 6.591239327064391e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0593, 'grad_norm': 0.08477400243282318, 'learning_rate': 6.146735013900173e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0486, 'grad_norm': 0.16341795027256012, 'learning_rate': 5.717271806664559e-06, 'epoch': 1.79}\n",
      "{'loss': 0.051, 'grad_norm': 0.03510883077979088, 'learning_rate': 5.302918531962464e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0328, 'grad_norm': 0.20696143805980682, 'learning_rate': 4.903741594851841e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0628, 'grad_norm': 0.1527072638273239, 'learning_rate': 4.519804968201313e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0472, 'grad_norm': 0.137675479054451, 'learning_rate': 4.151170182437924e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0635, 'grad_norm': 0.06608156114816666, 'learning_rate': 3.7978963156860446e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0341, 'grad_norm': 0.12641692161560059, 'learning_rate': 3.4600399842994237e-06, 'epoch': 1.84}\n",
      "{'loss': 0.061, 'grad_norm': 0.119529590010643, 'learning_rate': 3.13765533378777e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0418, 'grad_norm': 0.08178552985191345, 'learning_rate': 2.8307940301392164e-06, 'epoch': 1.85}\n",
      "{'loss': 0.056, 'grad_norm': 0.05063999816775322, 'learning_rate': 2.539505251540353e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0458, 'grad_norm': 0.20632289350032806, 'learning_rate': 2.263835680494686e-06, 'epoch': 1.87}\n",
      "{'loss': 0.069, 'grad_norm': 0.15244919061660767, 'learning_rate': 2.003829496341325e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0516, 'grad_norm': 0.14999669790267944, 'learning_rate': 1.7595283681746678e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0585, 'grad_norm': 0.13641268014907837, 'learning_rate': 1.5309714481664183e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0391, 'grad_norm': 0.10077538341283798, 'learning_rate': 1.3181953652910305e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0453, 'grad_norm': 0.05453069135546684, 'learning_rate': 1.121234219455447e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0395, 'grad_norm': 0.09250739216804504, 'learning_rate': 9.401195760341708e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0642, 'grad_norm': 0.18010622262954712, 'learning_rate': 7.748804608105675e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0493, 'grad_norm': 0.1298333704471588, 'learning_rate': 6.255433553250978e-07, 'epoch': 1.93}\n",
      "{'loss': 0.0522, 'grad_norm': 0.11278846114873886, 'learning_rate': 4.921321926313893e-07, 'epoch': 1.94}\n",
      "{'loss': 0.0285, 'grad_norm': 0.11762502044439316, 'learning_rate': 3.746683534606277e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0558, 'grad_norm': 0.1635393649339676, 'learning_rate': 2.7317066279506363e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0479, 'grad_norm': 0.12097316980361938, 'learning_rate': 1.8765538685108218e-07, 'epoch': 1.96}\n",
      "{'loss': 0.0633, 'grad_norm': 0.04246087744832039, 'learning_rate': 1.1813623047236544e-07, 'epoch': 1.97}\n",
      "{'loss': 0.0369, 'grad_norm': 0.07308633625507355, 'learning_rate': 6.462433493347187e-08, 'epoch': 1.98}\n",
      "{'loss': 0.0652, 'grad_norm': 0.04509366676211357, 'learning_rate': 2.712827615437563e-08, 'epoch': 1.99}\n",
      "{'loss': 0.0505, 'grad_norm': 0.23413556814193726, 'learning_rate': 5.654063326032688e-09, 'epoch': 1.99}\n",
      "{'train_runtime': 31975.4338, 'train_samples_per_second': 0.6, 'train_steps_per_second': 0.2, 'train_loss': 0.11068513335251226, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6396, training_loss=0.11068513335251226, metrics={'train_runtime': 31975.4338, 'train_samples_per_second': 0.6, 'train_steps_per_second': 0.2, 'total_flos': 8.107220558738227e+17, 'train_loss': 0.11068513335251226, 'epoch': 1.9997915581031789})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('StructLM-7B-luis_tokenizer/tokenizer_config.json',\n",
       " 'StructLM-7B-luis_tokenizer/special_tokens_map.json',\n",
       " 'StructLM-7B-luis_tokenizer/tokenizer.model',\n",
       " 'StructLM-7B-luis_tokenizer/added_tokens.json',\n",
       " 'StructLM-7B-luis_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model + \"_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
