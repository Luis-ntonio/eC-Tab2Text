{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "dataset_name = \"kokujin/prompts_1\"\n",
    "\n",
    "new_model = \"Meta-Llama-3-8B-luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{model_name}/\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 3\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 1000\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01df1210eeb24e44a159cf592e5ea102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb1d136400945bba54a48d0deaacfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91cba9a69734f348152386d2c42e477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37e2097d6e142dcb22e48a66e6633ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f30c2b0fc104c089aa15be4607d4fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bec23bcb6a466c840822b4e04c76e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e376a307bd1e42aeae975ef17d7d3376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d8d4cc82c543778387cd3f3e218a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9553e310647c47d09db93d2a509c94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b94d01b20340b08f7ddff890b437ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ec1ffc6b03416187677d959575bd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9d6cf9228b42998ca04d23fa4bf660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb62752c4e74c6dbd29086b7aee02c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3284, 'grad_norm': 0.16461588442325592, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.2095, 'grad_norm': 0.237275630235672, 'learning_rate': 5.208333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 0.7133, 'grad_norm': 0.2465115338563919, 'learning_rate': 7.8125e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3775, 'grad_norm': 0.5179443955421448, 'learning_rate': 0.00010416666666666667, 'epoch': 0.03}\n",
      "{'loss': 0.2465, 'grad_norm': 0.16486196219921112, 'learning_rate': 0.00013020833333333333, 'epoch': 0.04}\n",
      "{'loss': 0.2246, 'grad_norm': 0.3671402931213379, 'learning_rate': 0.00015625, 'epoch': 0.05}\n",
      "{'loss': 0.1762, 'grad_norm': 0.29935628175735474, 'learning_rate': 0.00018229166666666667, 'epoch': 0.05}\n",
      "{'loss': 0.2089, 'grad_norm': 0.2401837855577469, 'learning_rate': 0.00019999917944905216, 'epoch': 0.06}\n",
      "{'loss': 0.165, 'grad_norm': 0.12683165073394775, 'learning_rate': 0.00019998603811858571, 'epoch': 0.07}\n",
      "{'loss': 0.1902, 'grad_norm': 0.18932399153709412, 'learning_rate': 0.00019995687283209658, 'epoch': 0.08}\n",
      "{'loss': 0.1478, 'grad_norm': 0.114593505859375, 'learning_rate': 0.00019991168826367011, 'epoch': 0.09}\n",
      "{'loss': 0.1579, 'grad_norm': 0.16413745284080505, 'learning_rate': 0.00019985049165467257, 'epoch': 0.09}\n",
      "{'loss': 0.1304, 'grad_norm': 0.1298794448375702, 'learning_rate': 0.00019977329281259108, 'epoch': 0.1}\n",
      "{'loss': 0.1668, 'grad_norm': 0.23028533160686493, 'learning_rate': 0.00019968010410946154, 'epoch': 0.11}\n",
      "{'loss': 0.1287, 'grad_norm': 0.1291658729314804, 'learning_rate': 0.00019957094047988582, 'epoch': 0.12}\n",
      "{'loss': 0.1407, 'grad_norm': 0.21002337336540222, 'learning_rate': 0.00019944581941863857, 'epoch': 0.13}\n",
      "{'loss': 0.1215, 'grad_norm': 0.1347612887620926, 'learning_rate': 0.00019930476097786327, 'epoch': 0.13}\n",
      "{'loss': 0.1548, 'grad_norm': 0.3085630238056183, 'learning_rate': 0.0001991477877638587, 'epoch': 0.14}\n",
      "{'loss': 0.1316, 'grad_norm': 0.15279293060302734, 'learning_rate': 0.00019897492493345598, 'epoch': 0.15}\n",
      "{'loss': 0.1344, 'grad_norm': 0.12289269268512726, 'learning_rate': 0.00019878620018998696, 'epoch': 0.16}\n",
      "{'loss': 0.1158, 'grad_norm': 0.12440578639507294, 'learning_rate': 0.00019858164377884436, 'epoch': 0.16}\n",
      "{'loss': 0.1374, 'grad_norm': 0.13387973606586456, 'learning_rate': 0.00019836128848263465, 'epoch': 0.17}\n",
      "{'loss': 0.1059, 'grad_norm': 0.09615334868431091, 'learning_rate': 0.0001981251696159241, 'epoch': 0.18}\n",
      "{'loss': 0.1334, 'grad_norm': 0.15349221229553223, 'learning_rate': 0.00019787332501957941, 'epoch': 0.19}\n",
      "{'loss': 0.118, 'grad_norm': 0.13743956387043, 'learning_rate': 0.0001976057950547031, 'epoch': 0.2}\n",
      "{'loss': 0.1417, 'grad_norm': 0.13537879288196564, 'learning_rate': 0.00019732262259616523, 'epoch': 0.2}\n",
      "{'loss': 0.103, 'grad_norm': 0.1496138572692871, 'learning_rate': 0.0001970238530257322, 'epoch': 0.21}\n",
      "{'loss': 0.107, 'grad_norm': 0.1049102246761322, 'learning_rate': 0.00019670953422479368, 'epoch': 0.22}\n",
      "{'loss': 0.0909, 'grad_norm': 0.10400135070085526, 'learning_rate': 0.00019637971656668918, 'epoch': 0.23}\n",
      "{'loss': 0.1247, 'grad_norm': 0.22638899087905884, 'learning_rate': 0.0001960344529086351, 'epoch': 0.23}\n",
      "{'loss': 0.1132, 'grad_norm': 0.12680000066757202, 'learning_rate': 0.00019567379858325356, 'epoch': 0.24}\n",
      "{'loss': 0.113, 'grad_norm': 0.21582889556884766, 'learning_rate': 0.000195297811389705, 'epoch': 0.25}\n",
      "{'loss': 0.1005, 'grad_norm': 0.11152087152004242, 'learning_rate': 0.00019490655158442484, 'epoch': 0.26}\n",
      "{'loss': 0.1161, 'grad_norm': 0.115557000041008, 'learning_rate': 0.00019450008187146684, 'epoch': 0.27}\n",
      "{'loss': 0.1023, 'grad_norm': 0.12924060225486755, 'learning_rate': 0.00019407846739245415, 'epoch': 0.27}\n",
      "{'loss': 0.1145, 'grad_norm': 0.1220051571726799, 'learning_rate': 0.00019364177571613926, 'epoch': 0.28}\n",
      "{'loss': 0.099, 'grad_norm': 0.1157776266336441, 'learning_rate': 0.00019319007682757556, 'epoch': 0.29}\n",
      "{'loss': 0.0953, 'grad_norm': 0.15037314593791962, 'learning_rate': 0.0001927234431169014, 'epoch': 0.3}\n",
      "{'loss': 0.0785, 'grad_norm': 0.11677601933479309, 'learning_rate': 0.00019224194936773853, 'epoch': 0.3}\n",
      "{'loss': 0.0964, 'grad_norm': 0.1857115626335144, 'learning_rate': 0.00019174567274520728, 'epoch': 0.31}\n",
      "{'loss': 0.0766, 'grad_norm': 0.13954727351665497, 'learning_rate': 0.00019123469278355985, 'epoch': 0.32}\n",
      "{'loss': 0.0948, 'grad_norm': 0.19866567850112915, 'learning_rate': 0.00019070909137343408, 'epoch': 0.33}\n",
      "{'loss': 0.0758, 'grad_norm': 0.12323533743619919, 'learning_rate': 0.0001901689527487294, 'epoch': 0.34}\n",
      "{'loss': 0.0931, 'grad_norm': 0.1211128756403923, 'learning_rate': 0.0001896143634731074, 'epoch': 0.34}\n",
      "{'loss': 0.0843, 'grad_norm': 0.10732097178697586, 'learning_rate': 0.00018904541242611902, 'epoch': 0.35}\n",
      "{'loss': 0.1081, 'grad_norm': 0.18020175397396088, 'learning_rate': 0.00018846219078896037, 'epoch': 0.36}\n",
      "{'loss': 0.0859, 'grad_norm': 0.1062532439827919, 'learning_rate': 0.00018786479202986006, 'epoch': 0.37}\n",
      "{'loss': 0.0994, 'grad_norm': 0.15130512416362762, 'learning_rate': 0.0001872533118890997, 'epoch': 0.38}\n",
      "{'loss': 0.0788, 'grad_norm': 0.11984367668628693, 'learning_rate': 0.00018662784836367028, 'epoch': 0.38}\n",
      "{'loss': 0.0906, 'grad_norm': 0.12687352299690247, 'learning_rate': 0.00018598850169156722, 'epoch': 0.39}\n",
      "{'loss': 0.0824, 'grad_norm': 0.11364774405956268, 'learning_rate': 0.00018533537433572581, 'epoch': 0.4}\n",
      "{'loss': 0.0883, 'grad_norm': 0.18850114941596985, 'learning_rate': 0.00018466857096760046, 'epoch': 0.41}\n",
      "{'loss': 0.0786, 'grad_norm': 0.08917820453643799, 'learning_rate': 0.00018398819845038972, 'epoch': 0.41}\n",
      "{'loss': 0.0954, 'grad_norm': 0.2518673837184906, 'learning_rate': 0.0001832943658219103, 'epoch': 0.42}\n",
      "{'loss': 0.0752, 'grad_norm': 0.11351489275693893, 'learning_rate': 0.00018258718427712238, 'epoch': 0.43}\n",
      "{'loss': 0.0878, 'grad_norm': 0.18976137042045593, 'learning_rate': 0.00018186676715030924, 'epoch': 0.44}\n",
      "{'loss': 0.0777, 'grad_norm': 0.08672059327363968, 'learning_rate': 0.0001811332298969142, 'epoch': 0.45}\n",
      "{'loss': 0.1, 'grad_norm': 0.17647337913513184, 'learning_rate': 0.0001803866900750376, 'epoch': 0.45}\n",
      "{'loss': 0.0729, 'grad_norm': 0.10212672501802444, 'learning_rate': 0.0001796272673265963, 'epoch': 0.46}\n",
      "{'loss': 0.0809, 'grad_norm': 0.12904827296733856, 'learning_rate': 0.00017885508335815014, 'epoch': 0.47}\n",
      "{'loss': 0.071, 'grad_norm': 0.09842263162136078, 'learning_rate': 0.0001780702619213967, 'epoch': 0.48}\n",
      "{'loss': 0.0849, 'grad_norm': 0.44641193747520447, 'learning_rate': 0.0001772729287933387, 'epoch': 0.48}\n",
      "{'loss': 0.0724, 'grad_norm': 0.11430712789297104, 'learning_rate': 0.00017646321175612668, 'epoch': 0.49}\n",
      "{'loss': 0.0897, 'grad_norm': 0.18171948194503784, 'learning_rate': 0.00017564124057658056, 'epoch': 0.5}\n",
      "{'loss': 0.067, 'grad_norm': 0.11830407381057739, 'learning_rate': 0.00017480714698539266, 'epoch': 0.51}\n",
      "{'loss': 0.0766, 'grad_norm': 0.13029517233371735, 'learning_rate': 0.00017396106465601663, 'epoch': 0.52}\n",
      "{'loss': 0.0742, 'grad_norm': 0.0899861752986908, 'learning_rate': 0.0001731031291832444, 'epoch': 0.52}\n",
      "{'loss': 0.0824, 'grad_norm': 0.12723049521446228, 'learning_rate': 0.0001722334780614756, 'epoch': 0.53}\n",
      "{'loss': 0.0736, 'grad_norm': 0.10397988557815552, 'learning_rate': 0.00017135225066268255, 'epoch': 0.54}\n",
      "{'loss': 0.0823, 'grad_norm': 0.2177278697490692, 'learning_rate': 0.00017045958821407405, 'epoch': 0.55}\n",
      "{'loss': 0.0715, 'grad_norm': 0.09467151015996933, 'learning_rate': 0.00016955563377546207, 'epoch': 0.55}\n",
      "{'loss': 0.0812, 'grad_norm': 0.20167994499206543, 'learning_rate': 0.0001686405322163349, 'epoch': 0.56}\n",
      "{'loss': 0.0665, 'grad_norm': 0.12881746888160706, 'learning_rate': 0.00016771443019263983, 'epoch': 0.57}\n",
      "{'loss': 0.0745, 'grad_norm': 0.18849557638168335, 'learning_rate': 0.00016677747612327997, 'epoch': 0.58}\n",
      "{'loss': 0.067, 'grad_norm': 0.11694695800542831, 'learning_rate': 0.00016582982016632818, 'epoch': 0.59}\n",
      "{'loss': 0.0692, 'grad_norm': 0.10969908535480499, 'learning_rate': 0.00016487161419496263, 'epoch': 0.59}\n",
      "{'loss': 0.0586, 'grad_norm': 0.1440378725528717, 'learning_rate': 0.00016390301177312722, 'epoch': 0.6}\n",
      "{'loss': 0.0819, 'grad_norm': 0.18102584779262543, 'learning_rate': 0.00016292416813092105, 'epoch': 0.61}\n",
      "{'loss': 0.0666, 'grad_norm': 0.10760992020368576, 'learning_rate': 0.00016193524013972114, 'epoch': 0.62}\n",
      "{'loss': 0.0836, 'grad_norm': 0.18939076364040375, 'learning_rate': 0.00016093638628704167, 'epoch': 0.63}\n",
      "{'loss': 0.0521, 'grad_norm': 0.080052949488163, 'learning_rate': 0.0001599277666511347, 'epoch': 0.63}\n",
      "{'loss': 0.0776, 'grad_norm': 0.22862310707569122, 'learning_rate': 0.00015890954287533555, 'epoch': 0.64}\n",
      "{'loss': 0.0585, 'grad_norm': 0.1328810453414917, 'learning_rate': 0.00015788187814215764, 'epoch': 0.65}\n",
      "{'loss': 0.0781, 'grad_norm': 0.1510317474603653, 'learning_rate': 0.00015684493714714047, 'epoch': 0.66}\n",
      "{'loss': 0.0519, 'grad_norm': 0.1198466494679451, 'learning_rate': 0.00015579888607245517, 'epoch': 0.66}\n",
      "{'loss': 0.0783, 'grad_norm': 0.20313912630081177, 'learning_rate': 0.000154743892560272, 'epoch': 0.67}\n",
      "{'loss': 0.0502, 'grad_norm': 0.11452645808458328, 'learning_rate': 0.00015368012568589342, 'epoch': 0.68}\n",
      "{'loss': 0.0727, 'grad_norm': 0.14314495027065277, 'learning_rate': 0.00015260775593065802, 'epoch': 0.69}\n",
      "{'loss': 0.057, 'grad_norm': 0.06434272974729538, 'learning_rate': 0.00015152695515461865, 'epoch': 0.7}\n",
      "{'loss': 0.071, 'grad_norm': 0.1614643782377243, 'learning_rate': 0.00015043789656899988, 'epoch': 0.7}\n",
      "{'loss': 0.0486, 'grad_norm': 0.13058045506477356, 'learning_rate': 0.00014934075470843887, 'epoch': 0.71}\n",
      "{'loss': 0.0719, 'grad_norm': 0.18131287395954132, 'learning_rate': 0.00014823570540301408, 'epoch': 0.72}\n",
      "{'loss': 0.0547, 'grad_norm': 0.0917607769370079, 'learning_rate': 0.00014712292575006633, 'epoch': 0.73}\n",
      "{'loss': 0.0851, 'grad_norm': 0.18416103720664978, 'learning_rate': 0.00014600259408581687, 'epoch': 0.73}\n",
      "{'loss': 0.0515, 'grad_norm': 0.1413326859474182, 'learning_rate': 0.00014487488995678708, 'epoch': 0.74}\n",
      "{'loss': 0.0572, 'grad_norm': 0.13125142455101013, 'learning_rate': 0.00014373999409102362, 'epoch': 0.75}\n",
      "{'loss': 0.0553, 'grad_norm': 0.09877472370862961, 'learning_rate': 0.00014259808836913492, 'epoch': 0.76}\n",
      "{'loss': 0.081, 'grad_norm': 0.1793230026960373, 'learning_rate': 0.00014144935579514246, 'epoch': 0.77}\n",
      "{'loss': 0.0512, 'grad_norm': 0.12175089120864868, 'learning_rate': 0.00014029398046715223, 'epoch': 0.77}\n",
      "{'loss': 0.0558, 'grad_norm': 0.13215778768062592, 'learning_rate': 0.00013913214754785095, 'epoch': 0.78}\n",
      "{'loss': 0.0535, 'grad_norm': 0.09808023273944855, 'learning_rate': 0.00013796404323483132, 'epoch': 0.79}\n",
      "{'loss': 0.0545, 'grad_norm': 0.1036866307258606, 'learning_rate': 0.00013678985473075176, 'epoch': 0.8}\n",
      "{'loss': 0.0505, 'grad_norm': 0.08988085389137268, 'learning_rate': 0.00013560977021333497, 'epoch': 0.81}\n",
      "{'loss': 0.0761, 'grad_norm': 0.19154533743858337, 'learning_rate': 0.00013442397880521008, 'epoch': 0.81}\n",
      "{'loss': 0.0569, 'grad_norm': 0.12072727084159851, 'learning_rate': 0.0001332326705436037, 'epoch': 0.82}\n",
      "{'loss': 0.0628, 'grad_norm': 0.14147798717021942, 'learning_rate': 0.00013203603634988386, 'epoch': 0.83}\n",
      "{'loss': 0.0558, 'grad_norm': 0.12975828349590302, 'learning_rate': 0.000130834267998963, 'epoch': 0.84}\n",
      "{'loss': 0.0596, 'grad_norm': 0.19969676434993744, 'learning_rate': 0.00012962755808856342, 'epoch': 0.84}\n",
      "{'loss': 0.0451, 'grad_norm': 0.08512550592422485, 'learning_rate': 0.00012841610000835125, 'epoch': 0.85}\n",
      "{'loss': 0.066, 'grad_norm': 0.1487194150686264, 'learning_rate': 0.00012720008790894366, 'epoch': 0.86}\n",
      "{'loss': 0.0397, 'grad_norm': 0.11497163027524948, 'learning_rate': 0.00012597971667079361, 'epoch': 0.87}\n",
      "{'loss': 0.067, 'grad_norm': 0.1517353653907776, 'learning_rate': 0.0001247551818729582, 'epoch': 0.88}\n",
      "{'loss': 0.0441, 'grad_norm': 0.10019470006227493, 'learning_rate': 0.0001235266797617545, 'epoch': 0.88}\n",
      "{'loss': 0.0578, 'grad_norm': 0.11631037294864655, 'learning_rate': 0.00012229440721930906, 'epoch': 0.89}\n",
      "{'loss': 0.0458, 'grad_norm': 0.13269539177417755, 'learning_rate': 0.00012105856173200498, 'epoch': 0.9}\n",
      "{'loss': 0.0606, 'grad_norm': 0.14545674622058868, 'learning_rate': 0.00011981934135883237, 'epoch': 0.91}\n",
      "{'loss': 0.0549, 'grad_norm': 0.1214185357093811, 'learning_rate': 0.00011857694469964715, 'epoch': 0.91}\n",
      "{'loss': 0.0578, 'grad_norm': 0.16503901779651642, 'learning_rate': 0.00011733157086334294, 'epoch': 0.92}\n",
      "{'loss': 0.0465, 'grad_norm': 0.11620346456766129, 'learning_rate': 0.0001160834194359416, 'epoch': 0.93}\n",
      "{'loss': 0.0766, 'grad_norm': 0.16109348833560944, 'learning_rate': 0.00011483269044860705, 'epoch': 0.94}\n",
      "{'loss': 0.044, 'grad_norm': 0.12791192531585693, 'learning_rate': 0.000113579584345588, 'epoch': 0.95}\n",
      "{'loss': 0.0675, 'grad_norm': 0.12485138326883316, 'learning_rate': 0.0001123243019520942, 'epoch': 0.95}\n",
      "{'loss': 0.0448, 'grad_norm': 0.14500558376312256, 'learning_rate': 0.00011106704444211207, 'epoch': 0.96}\n",
      "{'loss': 0.0523, 'grad_norm': 0.14029914140701294, 'learning_rate': 0.000109808013306164, 'epoch': 0.97}\n",
      "{'loss': 0.0437, 'grad_norm': 0.12966245412826538, 'learning_rate': 0.00010854741031901703, 'epoch': 0.98}\n",
      "{'loss': 0.0751, 'grad_norm': 0.17107540369033813, 'learning_rate': 0.00010728543750734622, 'epoch': 0.98}\n",
      "{'loss': 0.0365, 'grad_norm': 0.09135702252388, 'learning_rate': 0.00010602229711735726, 'epoch': 0.99}\n",
      "{'loss': 0.0607, 'grad_norm': 0.08210639655590057, 'learning_rate': 0.00010475819158237425, 'epoch': 1.0}\n",
      "{'loss': 0.0339, 'grad_norm': 0.11569935828447342, 'learning_rate': 0.0001034933234903973, 'epoch': 1.01}\n",
      "{'loss': 0.0552, 'grad_norm': 0.14199353754520416, 'learning_rate': 0.0001022278955516354, 'epoch': 1.02}\n",
      "{'loss': 0.046, 'grad_norm': 0.05054362490773201, 'learning_rate': 0.00010096211056601958, 'epoch': 1.02}\n",
      "{'loss': 0.0535, 'grad_norm': 0.0896683856844902, 'learning_rate': 9.969617139070202e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0371, 'grad_norm': 0.12672972679138184, 'learning_rate': 9.84302809075455e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0542, 'grad_norm': 0.0811162069439888, 'learning_rate': 9.716464199060946e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0422, 'grad_norm': 0.14638996124267578, 'learning_rate': 9.589945747363667e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0551, 'grad_norm': 0.10969869047403336, 'learning_rate': 9.463493011754706e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0313, 'grad_norm': 0.10893837362527847, 'learning_rate': 9.337126257794255e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0406, 'grad_norm': 0.11091223359107971, 'learning_rate': 9.210865737262924e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0403, 'grad_norm': 0.07989752292633057, 'learning_rate': 9.084731684916151e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0476, 'grad_norm': 0.04963237792253494, 'learning_rate': 8.958744315241341e-05, 'epoch': 1.09}\n",
      "{'loss': 0.04, 'grad_norm': 0.0846157819032669, 'learning_rate': 8.832923819218238e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0421, 'grad_norm': 0.06332772970199585, 'learning_rate': 8.707290361083107e-05, 'epoch': 1.11}\n",
      "{'loss': 0.037, 'grad_norm': 0.09633447229862213, 'learning_rate': 8.581864075097144e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0389, 'grad_norm': 0.06561752408742905, 'learning_rate': 8.456665062319742e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0399, 'grad_norm': 0.0706453025341034, 'learning_rate': 8.33171338738706e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0526, 'grad_norm': 0.08662224560976028, 'learning_rate': 8.207029075296392e-05, 'epoch': 1.14}\n",
      "{'loss': 0.0347, 'grad_norm': 0.07682035863399506, 'learning_rate': 8.082632108196969e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0499, 'grad_norm': 0.08861769735813141, 'learning_rate': 7.958542422187538e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0318, 'grad_norm': 0.09843742847442627, 'learning_rate': 7.834779904121399e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0483, 'grad_norm': 0.09600204974412918, 'learning_rate': 7.711364388419278e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0333, 'grad_norm': 0.11180733889341354, 'learning_rate': 7.588315653890629e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0371, 'grad_norm': 0.06240876391530037, 'learning_rate': 7.465653420563845e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0349, 'grad_norm': 0.12681451439857483, 'learning_rate': 7.343397346525888e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0479, 'grad_norm': 0.09684491902589798, 'learning_rate': 7.221567024771849e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0342, 'grad_norm': 0.07758141309022903, 'learning_rate': 7.100181980064937e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0461, 'grad_norm': 0.14388200640678406, 'learning_rate': 6.979261665807389e-05, 'epoch': 1.22}\n",
      "{'loss': 0.0355, 'grad_norm': 0.030871519818902016, 'learning_rate': 6.858825460922849e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0421, 'grad_norm': 0.08993381261825562, 'learning_rate': 6.738892666750651e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0267, 'grad_norm': 0.0784207433462143, 'learning_rate': 6.619482503952559e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0447, 'grad_norm': 0.03166752681136131, 'learning_rate': 6.500614109432419e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0352, 'grad_norm': 0.11542058736085892, 'learning_rate': 6.382306533269238e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0408, 'grad_norm': 0.12935253977775574, 'learning_rate': 6.264578735664194e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0298, 'grad_norm': 0.07578512281179428, 'learning_rate': 6.147449583902036e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0383, 'grad_norm': 0.11626563221216202, 'learning_rate': 6.030937849327356e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0348, 'grad_norm': 0.20555254817008972, 'learning_rate': 5.91506220433629e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0507, 'grad_norm': 0.06619048118591309, 'learning_rate': 5.79984121938401e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0355, 'grad_norm': 0.04776697978377342, 'learning_rate': 5.6852933600086125e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0383, 'grad_norm': 0.11635833978652954, 'learning_rate': 5.5714369838717874e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0452, 'grad_norm': 0.17976954579353333, 'learning_rate': 5.4582903378167716e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0502, 'grad_norm': 0.09262017905712128, 'learning_rate': 5.3458715549440984e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0359, 'grad_norm': 0.12711788713932037, 'learning_rate': 5.234198651705527e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0355, 'grad_norm': 0.056767214089632034, 'learning_rate': 5.12328952501671e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0398, 'grad_norm': 0.1483386605978012, 'learning_rate': 5.013161949388993e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0414, 'grad_norm': 0.08915285021066666, 'learning_rate': 4.903833574080825e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0295, 'grad_norm': 0.046218086034059525, 'learning_rate': 4.795321920269279e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0397, 'grad_norm': 0.12822461128234863, 'learning_rate': 4.687644378242044e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0318, 'grad_norm': 0.04878208413720131, 'learning_rate': 4.580818204610458e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0466, 'grad_norm': 0.07453574985265732, 'learning_rate': 4.4748605195438976e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0325, 'grad_norm': 0.09416882693767548, 'learning_rate': 4.36978830402608e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0503, 'grad_norm': 0.08892672508955002, 'learning_rate': 4.265618397133674e-05, 'epoch': 1.41}\n",
      "{'loss': 0.032, 'grad_norm': 0.11152457445859909, 'learning_rate': 4.162367493337601e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0385, 'grad_norm': 0.052727147936820984, 'learning_rate': 4.060052139827582e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0262, 'grad_norm': 0.12512169778347015, 'learning_rate': 3.958688733860237e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0322, 'grad_norm': 0.14201666414737701, 'learning_rate': 3.858293520131221e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0303, 'grad_norm': 0.11524476110935211, 'learning_rate': 3.758882588171837e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0363, 'grad_norm': 0.045714642852544785, 'learning_rate': 3.660471869770474e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0282, 'grad_norm': 0.030757155269384384, 'learning_rate': 3.563077136419373e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0428, 'grad_norm': 0.08002441376447678, 'learning_rate': 3.466713996787039e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0326, 'grad_norm': 0.04550056532025337, 'learning_rate': 3.371397894216766e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0339, 'grad_norm': 0.07919441163539886, 'learning_rate': 3.277144104251669e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0245, 'grad_norm': 0.07870831340551376, 'learning_rate': 3.183967732186582e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0453, 'grad_norm': 0.05518612265586853, 'learning_rate': 3.0918837106472686e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0274, 'grad_norm': 0.05372694134712219, 'learning_rate': 3.0009067971972716e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0486, 'grad_norm': 0.11432696878910065, 'learning_rate': 2.9110515719728594e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0259, 'grad_norm': 0.026985235512256622, 'learning_rate': 2.8223324353463644e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0474, 'grad_norm': 0.07279114425182343, 'learning_rate': 2.73476360561837e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0259, 'grad_norm': 0.07583799958229065, 'learning_rate': 2.6483591167390407e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0415, 'grad_norm': 0.05602826550602913, 'learning_rate': 2.5631328160590318e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0248, 'grad_norm': 0.061818771064281464, 'learning_rate': 2.479098362110267e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0233, 'grad_norm': 0.11961507797241211, 'learning_rate': 2.3962692224170114e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0283, 'grad_norm': 0.019686441868543625, 'learning_rate': 2.3146586713375395e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0482, 'grad_norm': 0.03540588170289993, 'learning_rate': 2.2342797879367418e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0362, 'grad_norm': 0.10456182062625885, 'learning_rate': 2.155145453890076e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0456, 'grad_norm': 0.04659341648221016, 'learning_rate': 2.0772683514191004e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0312, 'grad_norm': 0.02461736649274826, 'learning_rate': 2.0006609612590142e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0427, 'grad_norm': 0.19347763061523438, 'learning_rate': 1.9253355606584655e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0312, 'grad_norm': 0.08293125033378601, 'learning_rate': 1.851304221411967e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0413, 'grad_norm': 0.02819814532995224, 'learning_rate': 1.778578807925253e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0315, 'grad_norm': 0.10299281775951385, 'learning_rate': 1.707170975313879e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0344, 'grad_norm': 0.10580477863550186, 'learning_rate': 1.6370921675353223e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0243, 'grad_norm': 0.13732372224330902, 'learning_rate': 1.568353615554985e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0433, 'grad_norm': 0.034432269632816315, 'learning_rate': 1.5009663355462655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.023, 'grad_norm': 0.11073943227529526, 'learning_rate': 1.4349411271251134e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0268, 'grad_norm': 0.10717105120420456, 'learning_rate': 1.3702885716192348e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0298, 'grad_norm': 0.016391411423683167, 'learning_rate': 1.3070190303723352e-05, 'epoch': 1.68}\n",
      "{'loss': 0.0317, 'grad_norm': 0.07435380667448044, 'learning_rate': 1.2451426430835733e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0241, 'grad_norm': 0.019834518432617188, 'learning_rate': 1.1846693261825525e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0466, 'grad_norm': 0.13283194601535797, 'learning_rate': 1.1256087712401087e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0275, 'grad_norm': 0.08504664897918701, 'learning_rate': 1.0679704434151016e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0327, 'grad_norm': 0.1391141265630722, 'learning_rate': 1.0117635799375291e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0246, 'grad_norm': 0.1497517079114914, 'learning_rate': 9.569971886281392e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0336, 'grad_norm': 0.13028590381145477, 'learning_rate': 9.036800464548157e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0299, 'grad_norm': 0.10657048970460892, 'learning_rate': 8.51820698125979e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0366, 'grad_norm': 0.13983257114887238, 'learning_rate': 8.014274547211808e-06, 'epoch': 1.75}\n",
      "{'loss': 0.0302, 'grad_norm': 0.07884363830089569, 'learning_rate': 7.525083923591592e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0297, 'grad_norm': 0.0314793661236763, 'learning_rate': 7.050713509035478e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0264, 'grad_norm': 0.03021439164876938, 'learning_rate': 6.591239327064391e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0385, 'grad_norm': 0.021956736221909523, 'learning_rate': 6.146735013900173e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0289, 'grad_norm': 0.08283203095197678, 'learning_rate': 5.717271806664559e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0315, 'grad_norm': 0.027988359332084656, 'learning_rate': 5.302918531962464e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0222, 'grad_norm': 0.13229899108409882, 'learning_rate': 4.903741594851841e-06, 'epoch': 1.81}\n",
      "{'loss': 0.039, 'grad_norm': 0.14821329712867737, 'learning_rate': 4.519804968201313e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0246, 'grad_norm': 0.03192727640271187, 'learning_rate': 4.151170182437924e-06, 'epoch': 1.82}\n",
      "{'loss': 0.045, 'grad_norm': 0.02651105634868145, 'learning_rate': 3.7978963156860446e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0238, 'grad_norm': 0.09901151806116104, 'learning_rate': 3.4600399842994237e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0355, 'grad_norm': 0.08254449814558029, 'learning_rate': 3.13765533378777e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0224, 'grad_norm': 0.12885339558124542, 'learning_rate': 2.8307940301392164e-06, 'epoch': 1.85}\n",
      "{'loss': 0.0402, 'grad_norm': 0.02186848595738411, 'learning_rate': 2.539505251540353e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0251, 'grad_norm': 0.08253265917301178, 'learning_rate': 2.263835680494686e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0424, 'grad_norm': 0.13156944513320923, 'learning_rate': 2.003829496341325e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0264, 'grad_norm': 0.1381448358297348, 'learning_rate': 1.7595283681746678e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0334, 'grad_norm': 0.12583468854427338, 'learning_rate': 1.5309714481664183e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0265, 'grad_norm': 0.032318584620952606, 'learning_rate': 1.3181953652910305e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0237, 'grad_norm': 0.09280703961849213, 'learning_rate': 1.121234219455447e-06, 'epoch': 1.91}\n",
      "{'loss': 0.0276, 'grad_norm': 0.13963216543197632, 'learning_rate': 9.401195760341708e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0297, 'grad_norm': 0.045011695474386215, 'learning_rate': 7.748804608105675e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0283, 'grad_norm': 0.06882579624652863, 'learning_rate': 6.255433553250978e-07, 'epoch': 1.93}\n",
      "{'loss': 0.0296, 'grad_norm': 0.08779445290565491, 'learning_rate': 4.921321926313893e-07, 'epoch': 1.94}\n",
      "{'loss': 0.0177, 'grad_norm': 0.03133805841207504, 'learning_rate': 3.746683534606277e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0361, 'grad_norm': 0.14508827030658722, 'learning_rate': 2.7317066279506363e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0306, 'grad_norm': 0.05794642120599747, 'learning_rate': 1.8765538685108218e-07, 'epoch': 1.96}\n",
      "{'loss': 0.0406, 'grad_norm': 0.12155400216579437, 'learning_rate': 1.1813623047236544e-07, 'epoch': 1.97}\n",
      "{'loss': 0.026, 'grad_norm': 0.06428957730531693, 'learning_rate': 6.462433493347187e-08, 'epoch': 1.98}\n",
      "{'loss': 0.0363, 'grad_norm': 0.04853217676281929, 'learning_rate': 2.712827615437563e-08, 'epoch': 1.99}\n",
      "{'loss': 0.0334, 'grad_norm': 0.10877735167741776, 'learning_rate': 5.654063326032688e-09, 'epoch': 1.99}\n",
      "{'train_runtime': 29068.7238, 'train_samples_per_second': 0.66, 'train_steps_per_second': 0.22, 'train_loss': 0.07449624132017854, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6396, training_loss=0.07449624132017854, metrics={'train_runtime': 29068.7238, 'train_samples_per_second': 0.66, 'train_steps_per_second': 0.22, 'total_flos': 7.535116752308797e+17, 'train_loss': 0.07449624132017854, 'epoch': 1.9997915581031789})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Meta-Llama-3-8B-luis_tokenizer/tokenizer_config.json',\n",
       " 'Meta-Llama-3-8B-luis_tokenizer/special_tokens_map.json',\n",
       " 'Meta-Llama-3-8B-luis_tokenizer/tokenizer.model',\n",
       " 'Meta-Llama-3-8B-luis_tokenizer/added_tokens.json',\n",
       " 'Meta-Llama-3-8B-luis_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model + \"_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
