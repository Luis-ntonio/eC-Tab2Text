{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TIGER-Lab/StructLM-7B\"\n",
    "\n",
    "dataset_name = \"kokujin/prompts_1\"\n",
    "\n",
    "new_model = \"StructLM-7B-luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{model_name}/\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 3\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 1200\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d507a119df64221a550a84f487f5b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb70be97bd0435bb7603d6fe466ac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987f5cb7759346dbbac5dd59dea244a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4107, 'grad_norm': 0.2513882517814636, 'learning_rate': 2.4509803921568626e-05, 'epoch': 0.0}\n",
      "{'loss': 1.339, 'grad_norm': 0.151072159409523, 'learning_rate': 4.901960784313725e-05, 'epoch': 0.01}\n",
      "{'loss': 0.9962, 'grad_norm': 0.30208277702331543, 'learning_rate': 7.352941176470589e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5361, 'grad_norm': 0.32670092582702637, 'learning_rate': 9.80392156862745e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3995, 'grad_norm': 0.15247255563735962, 'learning_rate': 0.00012254901960784316, 'epoch': 0.02}\n",
      "{'loss': 0.3467, 'grad_norm': 0.2853965759277344, 'learning_rate': 0.00014705882352941178, 'epoch': 0.02}\n",
      "{'loss': 0.3096, 'grad_norm': 0.1253095120191574, 'learning_rate': 0.0001715686274509804, 'epoch': 0.03}\n",
      "{'loss': 0.3301, 'grad_norm': 0.26811644434928894, 'learning_rate': 0.000196078431372549, 'epoch': 0.03}\n",
      "{'loss': 0.2839, 'grad_norm': 0.137332946062088, 'learning_rate': 0.00019999496906143586, 'epoch': 0.03}\n",
      "{'loss': 0.2993, 'grad_norm': 0.1554502546787262, 'learning_rate': 0.00019997586139008558, 'epoch': 0.04}\n",
      "{'loss': 0.2689, 'grad_norm': 0.1485118716955185, 'learning_rate': 0.0001999424971808411, 'epoch': 0.04}\n",
      "{'loss': 0.2708, 'grad_norm': 0.25323784351348877, 'learning_rate': 0.000199894881191432, 'epoch': 0.04}\n",
      "{'loss': 0.2505, 'grad_norm': 0.12394127994775772, 'learning_rate': 0.00019983302021188879, 'epoch': 0.05}\n",
      "{'loss': 0.2641, 'grad_norm': 0.11151297390460968, 'learning_rate': 0.0001997569230635749, 'epoch': 0.05}\n",
      "{'loss': 0.2415, 'grad_norm': 0.14551739394664764, 'learning_rate': 0.00019966660059792847, 'epoch': 0.06}\n",
      "{'loss': 0.2306, 'grad_norm': 0.16067002713680267, 'learning_rate': 0.00019956206569491508, 'epoch': 0.06}\n",
      "{'loss': 0.2239, 'grad_norm': 0.1364891678094864, 'learning_rate': 0.0001994433332611911, 'epoch': 0.06}\n",
      "{'loss': 0.2514, 'grad_norm': 0.25957000255584717, 'learning_rate': 0.00019931042022797786, 'epoch': 0.07}\n",
      "{'loss': 0.2339, 'grad_norm': 0.1495862752199173, 'learning_rate': 0.00019916334554864737, 'epoch': 0.07}\n",
      "{'loss': 0.2233, 'grad_norm': 0.1528075933456421, 'learning_rate': 0.0001990021301960196, 'epoch': 0.07}\n",
      "{'loss': 0.2196, 'grad_norm': 0.13398881256580353, 'learning_rate': 0.00019882679715937162, 'epoch': 0.08}\n",
      "{'loss': 0.2243, 'grad_norm': 0.2667725682258606, 'learning_rate': 0.00019863737144115958, 'epoch': 0.08}\n",
      "{'loss': 0.2052, 'grad_norm': 0.15233927965164185, 'learning_rate': 0.00019843388005345307, 'epoch': 0.08}\n",
      "{'loss': 0.2132, 'grad_norm': 0.14386773109436035, 'learning_rate': 0.00019821635201408338, 'epoch': 0.09}\n",
      "{'loss': 0.2075, 'grad_norm': 0.137682244181633, 'learning_rate': 0.00019798481834250556, 'epoch': 0.09}\n",
      "{'loss': 0.2088, 'grad_norm': 0.17559456825256348, 'learning_rate': 0.00019773931205537497, 'epoch': 0.1}\n",
      "{'loss': 0.1977, 'grad_norm': 0.1289295107126236, 'learning_rate': 0.00019747986816183924, 'epoch': 0.1}\n",
      "{'loss': 0.197, 'grad_norm': 0.27872687578201294, 'learning_rate': 0.00019720652365854576, 'epoch': 0.1}\n",
      "{'loss': 0.1963, 'grad_norm': 0.10833773761987686, 'learning_rate': 0.00019691931752436625, 'epoch': 0.11}\n",
      "{'loss': 0.1993, 'grad_norm': 0.18600381910800934, 'learning_rate': 0.00019661829071483816, 'epoch': 0.11}\n",
      "{'loss': 0.1964, 'grad_norm': 0.12115596234798431, 'learning_rate': 0.00019630348615632445, 'epoch': 0.11}\n",
      "{'loss': 0.1885, 'grad_norm': 0.17409853637218475, 'learning_rate': 0.00019597494873989238, 'epoch': 0.12}\n",
      "{'loss': 0.1944, 'grad_norm': 0.14686571061611176, 'learning_rate': 0.00019563272531491208, 'epoch': 0.12}\n",
      "{'loss': 0.1832, 'grad_norm': 0.18886221945285797, 'learning_rate': 0.00019527686468237562, 'epoch': 0.13}\n",
      "{'loss': 0.189, 'grad_norm': 0.12913863360881805, 'learning_rate': 0.00019490741758793824, 'epoch': 0.13}\n",
      "{'loss': 0.1739, 'grad_norm': 0.15478511154651642, 'learning_rate': 0.00019452443671468192, 'epoch': 0.13}\n",
      "{'loss': 0.175, 'grad_norm': 0.13076847791671753, 'learning_rate': 0.00019412797667560283, 'epoch': 0.14}\n",
      "{'loss': 0.1683, 'grad_norm': 0.3541717231273651, 'learning_rate': 0.00019371809400582342, 'epoch': 0.14}\n",
      "{'loss': 0.1725, 'grad_norm': 0.1188606321811676, 'learning_rate': 0.0001932948471545307, 'epoch': 0.14}\n",
      "{'loss': 0.1666, 'grad_norm': 0.18312330543994904, 'learning_rate': 0.0001928582964766412, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2df00b40-75c4-405b-8b96-28fe705e3fcf)') - silently ignoring the lookup for the file config.json in TIGER-Lab/StructLM-7B.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in TIGER-Lab/StructLM-7B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1693, 'grad_norm': 0.14177587628364563, 'learning_rate': 0.00019240850422419457, 'epoch': 0.15}\n",
      "{'loss': 0.1908, 'grad_norm': 0.3106236457824707, 'learning_rate': 0.00019194553453747627, 'epoch': 0.15}\n",
      "{'loss': 0.1878, 'grad_norm': 0.1418384313583374, 'learning_rate': 0.00019146945343587128, 'epoch': 0.16}\n",
      "{'loss': 0.1644, 'grad_norm': 0.15781231224536896, 'learning_rate': 0.00019098032880844975, 'epoch': 0.16}\n",
      "{'loss': 0.1539, 'grad_norm': 0.13516023755073547, 'learning_rate': 0.000190478230404286, 'epoch': 0.17}\n",
      "{'loss': 0.175, 'grad_norm': 0.16053356230258942, 'learning_rate': 0.00018996322982251234, 'epoch': 0.17}\n",
      "{'loss': 0.1925, 'grad_norm': 0.14316949248313904, 'learning_rate': 0.00018943540050210894, 'epoch': 0.17}\n",
      "{'loss': 0.155, 'grad_norm': 0.18293477594852448, 'learning_rate': 0.00018889481771143166, 'epoch': 0.18}\n",
      "{'loss': 0.1608, 'grad_norm': 0.12895315885543823, 'learning_rate': 0.00018834155853747868, 'epoch': 0.18}\n",
      "{'loss': 0.1578, 'grad_norm': 0.21868377923965454, 'learning_rate': 0.0001877757018748978, 'epoch': 0.18}\n",
      "{'loss': 0.1674, 'grad_norm': 0.12589679658412933, 'learning_rate': 0.00018719732841473622, 'epoch': 0.19}\n",
      "{'loss': 0.1686, 'grad_norm': 0.2034291923046112, 'learning_rate': 0.00018660652063293406, 'epoch': 0.19}\n",
      "{'loss': 0.1393, 'grad_norm': 0.14033830165863037, 'learning_rate': 0.00018600336277856317, 'epoch': 0.2}\n",
      "{'loss': 0.1535, 'grad_norm': 0.18314185738563538, 'learning_rate': 0.00018538794086181326, 'epoch': 0.2}\n",
      "{'loss': 0.1501, 'grad_norm': 0.12556031346321106, 'learning_rate': 0.00018476034264172693, 'epoch': 0.2}\n",
      "{'loss': 0.1414, 'grad_norm': 0.1374007910490036, 'learning_rate': 0.00018412065761368518, 'epoch': 0.21}\n",
      "{'loss': 0.1417, 'grad_norm': 0.13562510907649994, 'learning_rate': 0.0001834689769966454, 'epoch': 0.21}\n",
      "{'loss': 0.1753, 'grad_norm': 0.17915169894695282, 'learning_rate': 0.00018280539372013367, 'epoch': 0.21}\n",
      "{'loss': 0.1429, 'grad_norm': 0.13288038969039917, 'learning_rate': 0.00018213000241099278, 'epoch': 0.22}\n",
      "{'loss': 0.1581, 'grad_norm': 0.19899697601795197, 'learning_rate': 0.00018144289937988882, 'epoch': 0.22}\n",
      "{'loss': 0.1342, 'grad_norm': 0.0976700633764267, 'learning_rate': 0.00018074418260757686, 'epoch': 0.22}\n",
      "{'loss': 0.1354, 'grad_norm': 0.1851351261138916, 'learning_rate': 0.00018003395173092938, 'epoch': 0.23}\n",
      "{'loss': 0.1217, 'grad_norm': 0.11605877429246902, 'learning_rate': 0.00017931230802872765, 'epoch': 0.23}\n",
      "{'loss': 0.1404, 'grad_norm': 0.14733938872814178, 'learning_rate': 0.00017857935440721974, 'epoch': 0.24}\n",
      "{'loss': 0.1403, 'grad_norm': 0.19132716953754425, 'learning_rate': 0.00017783519538544588, 'epoch': 0.24}\n",
      "{'loss': 0.1271, 'grad_norm': 0.16278749704360962, 'learning_rate': 0.00017707993708033414, 'epoch': 0.24}\n",
      "{'loss': 0.1414, 'grad_norm': 0.1754896640777588, 'learning_rate': 0.00017631368719156815, 'epoch': 0.25}\n",
      "{'loss': 0.1393, 'grad_norm': 0.18917806446552277, 'learning_rate': 0.0001755365549862293, 'epoch': 0.25}\n",
      "{'loss': 0.1349, 'grad_norm': 0.13455882668495178, 'learning_rate': 0.00017474865128321504, 'epoch': 0.25}\n",
      "{'loss': 0.1509, 'grad_norm': 0.12589336931705475, 'learning_rate': 0.00017395008843743627, 'epoch': 0.26}\n",
      "{'loss': 0.1203, 'grad_norm': 0.15557655692100525, 'learning_rate': 0.00017314098032379552, 'epoch': 0.26}\n",
      "{'loss': 0.1451, 'grad_norm': 0.1883644461631775, 'learning_rate': 0.00017232144232094842, 'epoch': 0.27}\n",
      "{'loss': 0.1346, 'grad_norm': 0.13715612888336182, 'learning_rate': 0.00017149159129485074, 'epoch': 0.27}\n",
      "{'loss': 0.1389, 'grad_norm': 0.11683006584644318, 'learning_rate': 0.00017065154558209327, 'epoch': 0.27}\n",
      "{'loss': 0.1285, 'grad_norm': 0.12590673565864563, 'learning_rate': 0.00016980142497302714, 'epoch': 0.28}\n",
      "{'loss': 0.1389, 'grad_norm': 0.16388066112995148, 'learning_rate': 0.0001689413506946816, 'epoch': 0.28}\n",
      "{'loss': 0.1101, 'grad_norm': 0.10327335447072983, 'learning_rate': 0.00016807144539347722, 'epoch': 0.28}\n",
      "{'loss': 0.1308, 'grad_norm': 0.19425132870674133, 'learning_rate': 0.00016719183311773644, 'epoch': 0.29}\n",
      "{'loss': 0.1078, 'grad_norm': 0.13843251764774323, 'learning_rate': 0.0001663026392999944, 'epoch': 0.29}\n",
      "{'loss': 0.1192, 'grad_norm': 0.1579984575510025, 'learning_rate': 0.0001654039907391121, 'epoch': 0.29}\n",
      "{'loss': 0.1097, 'grad_norm': 0.14110320806503296, 'learning_rate': 0.00016449601558219507, 'epoch': 0.3}\n",
      "{'loss': 0.1422, 'grad_norm': 0.17114531993865967, 'learning_rate': 0.00016357884330631968, 'epoch': 0.3}\n",
      "{'loss': 0.1178, 'grad_norm': 0.11642542481422424, 'learning_rate': 0.00016265260470006945, 'epoch': 0.31}\n",
      "{'loss': 0.1244, 'grad_norm': 0.11133604496717453, 'learning_rate': 0.00016171743184488494, 'epoch': 0.31}\n",
      "{'loss': 0.1162, 'grad_norm': 0.13798940181732178, 'learning_rate': 0.0001607734580962288, 'epoch': 0.31}\n",
      "{'loss': 0.1122, 'grad_norm': 0.1529083549976349, 'learning_rate': 0.0001598208180645693, 'epoch': 0.32}\n",
      "{'loss': 0.1183, 'grad_norm': 0.12444685399532318, 'learning_rate': 0.0001588596475961849, 'epoch': 0.32}\n",
      "{'loss': 0.1262, 'grad_norm': 0.13938476145267487, 'learning_rate': 0.00015789008375379274, 'epoch': 0.32}\n",
      "{'loss': 0.1117, 'grad_norm': 0.12124417722225189, 'learning_rate': 0.00015691226479700338, 'epoch': 0.33}\n",
      "{'loss': 0.1264, 'grad_norm': 0.1874418407678604, 'learning_rate': 0.00015592633016260502, 'epoch': 0.33}\n",
      "{'loss': 0.1104, 'grad_norm': 0.11533527821302414, 'learning_rate': 0.00015493242044467987, 'epoch': 0.34}\n",
      "{'loss': 0.1362, 'grad_norm': 0.1814139187335968, 'learning_rate': 0.00015393067737455544, 'epoch': 0.34}\n",
      "{'loss': 0.1, 'grad_norm': 0.15491190552711487, 'learning_rate': 0.0001529212438005938, 'epoch': 0.34}\n",
      "{'loss': 0.1111, 'grad_norm': 0.1188826933503151, 'learning_rate': 0.00015190426366782123, 'epoch': 0.35}\n",
      "{'loss': 0.1102, 'grad_norm': 0.11673880368471146, 'learning_rate': 0.00015087988199740185, 'epoch': 0.35}\n",
      "{'loss': 0.1316, 'grad_norm': 0.23044148087501526, 'learning_rate': 0.00014984824486595749, 'epoch': 0.35}\n",
      "{'loss': 0.0957, 'grad_norm': 0.16988128423690796, 'learning_rate': 0.00014880949938473744, 'epoch': 0.36}\n",
      "{'loss': 0.1332, 'grad_norm': 0.3266346752643585, 'learning_rate': 0.00014776379367864006, 'epoch': 0.36}\n",
      "{'loss': 0.1024, 'grad_norm': 0.14724712073802948, 'learning_rate': 0.00014671127686509053, 'epoch': 0.36}\n",
      "{'loss': 0.1061, 'grad_norm': 0.15305469930171967, 'learning_rate': 0.0001456520990327765, 'epoch': 0.37}\n",
      "{'loss': 0.1056, 'grad_norm': 0.1814563125371933, 'learning_rate': 0.00014458641122024561, 'epoch': 0.37}\n",
      "{'loss': 0.0981, 'grad_norm': 0.1752575784921646, 'learning_rate': 0.0001435143653943674, 'epoch': 0.38}\n",
      "{'loss': 0.1046, 'grad_norm': 0.11673986911773682, 'learning_rate': 0.00014243611442866282, 'epoch': 0.38}\n",
      "{'loss': 0.1043, 'grad_norm': 0.21915027499198914, 'learning_rate': 0.0001413518120815045, 'epoch': 0.38}\n",
      "{'loss': 0.0986, 'grad_norm': 0.1266040951013565, 'learning_rate': 0.00014026161297419094, 'epoch': 0.39}\n",
      "{'loss': 0.1119, 'grad_norm': 0.10239622741937637, 'learning_rate': 0.00013916567256889732, 'epoch': 0.39}\n",
      "{'loss': 0.0854, 'grad_norm': 0.1318577229976654, 'learning_rate': 0.00013806414714650692, 'epoch': 0.39}\n",
      "{'loss': 0.1227, 'grad_norm': 0.11723609268665314, 'learning_rate': 0.00013695719378432523, 'epoch': 0.4}\n",
      "{'loss': 0.0915, 'grad_norm': 0.17964348196983337, 'learning_rate': 0.00013584497033368095, 'epoch': 0.4}\n",
      "{'loss': 0.0863, 'grad_norm': 0.1639384776353836, 'learning_rate': 0.0001347276353974163, 'epoch': 0.41}\n",
      "{'loss': 0.0971, 'grad_norm': 0.12713702023029327, 'learning_rate': 0.00013360534830727033, 'epoch': 0.41}\n",
      "{'loss': 0.1158, 'grad_norm': 0.19293995201587677, 'learning_rate': 0.00013247826910115827, 'epoch': 0.41}\n",
      "{'loss': 0.0845, 'grad_norm': 0.19014202058315277, 'learning_rate': 0.00013134655850035015, 'epoch': 0.42}\n",
      "{'loss': 0.1041, 'grad_norm': 0.17002445459365845, 'learning_rate': 0.00013021037788655187, 'epoch': 0.42}\n",
      "{'loss': 0.0989, 'grad_norm': 0.1758316457271576, 'learning_rate': 0.00012906988927889235, 'epoch': 0.42}\n",
      "{'loss': 0.0955, 'grad_norm': 0.2301502525806427, 'learning_rate': 0.0001279252553108196, 'epoch': 0.43}\n",
      "{'loss': 0.077, 'grad_norm': 0.15021438896656036, 'learning_rate': 0.00012677663920690919, 'epoch': 0.43}\n",
      "{'loss': 0.1096, 'grad_norm': 0.1991192251443863, 'learning_rate': 0.0001256242047595884, 'epoch': 0.43}\n",
      "{'loss': 0.0802, 'grad_norm': 0.11204929649829865, 'learning_rate': 0.00012446811630577962, 'epoch': 0.44}\n",
      "{'loss': 0.1032, 'grad_norm': 0.18109828233718872, 'learning_rate': 0.00012330853870346574, 'epoch': 0.44}\n",
      "{'loss': 0.0814, 'grad_norm': 0.1238875463604927, 'learning_rate': 0.0001221456373081816, 'epoch': 0.45}\n",
      "{'loss': 0.0938, 'grad_norm': 0.1824694275856018, 'learning_rate': 0.00012097957794943417, 'epoch': 0.45}\n",
      "{'loss': 0.0916, 'grad_norm': 0.15335015952587128, 'learning_rate': 0.00011981052690705556, 'epoch': 0.45}\n",
      "{'loss': 0.0963, 'grad_norm': 0.13725467026233673, 'learning_rate': 0.00011863865088749113, 'epoch': 0.46}\n",
      "{'loss': 0.0795, 'grad_norm': 0.13781596720218658, 'learning_rate': 0.0001174641170000276, 'epoch': 0.46}\n",
      "{'loss': 0.0994, 'grad_norm': 0.1707666665315628, 'learning_rate': 0.00011628709273296308, 'epoch': 0.46}\n",
      "{'loss': 0.0955, 'grad_norm': 0.17458584904670715, 'learning_rate': 0.00011510774592972322, 'epoch': 0.47}\n",
      "{'loss': 0.0905, 'grad_norm': 0.14735597372055054, 'learning_rate': 0.00011392624476492704, 'epoch': 0.47}\n",
      "{'loss': 0.0915, 'grad_norm': 0.13850118219852448, 'learning_rate': 0.0001127427577204049, 'epoch': 0.48}\n",
      "{'loss': 0.097, 'grad_norm': 0.12391677498817444, 'learning_rate': 0.00011155745356117338, 'epoch': 0.48}\n",
      "{'loss': 0.0817, 'grad_norm': 0.100923553109169, 'learning_rate': 0.00011037050131136914, 'epoch': 0.48}\n",
      "{'loss': 0.1045, 'grad_norm': 0.2165144830942154, 'learning_rate': 0.00010918207023014626, 'epoch': 0.49}\n",
      "{'loss': 0.0854, 'grad_norm': 0.1275629848241806, 'learning_rate': 0.00010799232978753992, 'epoch': 0.49}\n",
      "{'loss': 0.092, 'grad_norm': 0.16383829712867737, 'learning_rate': 0.00010680144964029994, 'epoch': 0.49}\n",
      "{'loss': 0.0979, 'grad_norm': 0.09781758487224579, 'learning_rate': 0.00010560959960769779, 'epoch': 0.5}\n",
      "{'loss': 0.0902, 'grad_norm': 0.24037402868270874, 'learning_rate': 0.00010441694964731041, 'epoch': 0.5}\n",
      "{'loss': 0.0755, 'grad_norm': 0.11884407699108124, 'learning_rate': 0.00010322366983078426, 'epoch': 0.51}\n",
      "{'loss': 0.0944, 'grad_norm': 0.19164849817752838, 'learning_rate': 0.0001020299303195832, 'epoch': 0.51}\n",
      "{'loss': 0.0847, 'grad_norm': 0.1233486458659172, 'learning_rate': 0.00010083590134072345, 'epoch': 0.51}\n",
      "{'loss': 0.0868, 'grad_norm': 0.17887753248214722, 'learning_rate': 9.964175316249921e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0838, 'grad_norm': 0.14342939853668213, 'learning_rate': 9.844765607020256e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0839, 'grad_norm': 0.18630442023277283, 'learning_rate': 9.725378034184068e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0779, 'grad_norm': 0.14014141261577606, 'learning_rate': 9.606029622385432e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0859, 'grad_norm': 0.17978988587856293, 'learning_rate': 9.48673739068407e-05, 'epoch': 0.53}\n",
      "{'loss': 0.07, 'grad_norm': 0.11511407047510147, 'learning_rate': 9.367518350128424e-05, 'epoch': 0.53}\n",
      "{'loss': 0.0776, 'grad_norm': 0.22323627769947052, 'learning_rate': 9.248389501329904e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0698, 'grad_norm': 0.12321475148200989, 'learning_rate': 9.129367832038586e-05, 'epoch': 0.54}\n",
      "{'loss': 0.0826, 'grad_norm': 0.290785014629364, 'learning_rate': 9.010470314720775e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0778, 'grad_norm': 0.08284087479114532, 'learning_rate': 8.891713904138721e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0917, 'grad_norm': 0.14962227642536163, 'learning_rate': 8.773115534932897e-05, 'epoch': 0.55}\n",
      "{'loss': 0.0726, 'grad_norm': 0.18918877840042114, 'learning_rate': 8.654692119207086e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0861, 'grad_norm': 0.18957754969596863, 'learning_rate': 8.536460544116758e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0731, 'grad_norm': 0.14101120829582214, 'learning_rate': 8.418437669460943e-05, 'epoch': 0.56}\n",
      "{'loss': 0.0692, 'grad_norm': 0.18355901539325714, 'learning_rate': 8.300640325278019e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0786, 'grad_norm': 0.12319022417068481, 'learning_rate': 8.18308530944578e-05, 'epoch': 0.57}\n",
      "{'loss': 0.0781, 'grad_norm': 0.08616543561220169, 'learning_rate': 8.065789385286031e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0728, 'grad_norm': 0.134176105260849, 'learning_rate': 7.948769279174166e-05, 'epoch': 0.58}\n",
      "{'loss': 0.0801, 'grad_norm': 0.17940399050712585, 'learning_rate': 7.832041678153978e-05, 'epoch': 0.58}\n",
      "{'loss': 0.074, 'grad_norm': 0.12122584134340286, 'learning_rate': 7.715623227558088e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0773, 'grad_norm': 0.18427734076976776, 'learning_rate': 7.599530528634334e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0652, 'grad_norm': 0.11065846681594849, 'learning_rate': 7.483780136178428e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0679, 'grad_norm': 0.21441994607448578, 'learning_rate': 7.368388556173248e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0687, 'grad_norm': 0.12666508555412292, 'learning_rate': 7.253372243435087e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0847, 'grad_norm': 0.21546006202697754, 'learning_rate': 7.138747599267203e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0625, 'grad_norm': 0.11522198468446732, 'learning_rate': 7.024530969120991e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0817, 'grad_norm': 0.1705438643693924, 'learning_rate': 6.910738640265133e-05, 'epoch': 0.61}\n",
      "{'loss': 0.0817, 'grad_norm': 0.15728670358657837, 'learning_rate': 6.797386839463022e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0893, 'grad_norm': 0.18736061453819275, 'learning_rate': 6.684491730658854e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0727, 'grad_norm': 0.1211119145154953, 'learning_rate': 6.572069412672615e-05, 'epoch': 0.62}\n",
      "{'loss': 0.0743, 'grad_norm': 0.20449647307395935, 'learning_rate': 6.460135916904437e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0664, 'grad_norm': 0.1424882858991623, 'learning_rate': 6.348707205048492e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0743, 'grad_norm': 0.18194429576396942, 'learning_rate': 6.237799166816862e-05, 'epoch': 0.63}\n",
      "{'loss': 0.0603, 'grad_norm': 0.14282478392124176, 'learning_rate': 6.12742761767368e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0807, 'grad_norm': 0.29459303617477417, 'learning_rate': 6.01760829657984e-05, 'epoch': 0.64}\n",
      "{'loss': 0.0645, 'grad_norm': 0.03881194069981575, 'learning_rate': 5.908356863748628e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0674, 'grad_norm': 0.2834728956222534, 'learning_rate': 5.799688898412572e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0772, 'grad_norm': 0.2322637438774109, 'learning_rate': 5.691619896601865e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0892, 'grad_norm': 0.17238731682300568, 'learning_rate': 5.584165268934598e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0649, 'grad_norm': 0.18022297322750092, 'learning_rate': 5.4773403384192543e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0659, 'grad_norm': 0.2683214247226715, 'learning_rate': 5.371160338269611e-05, 'epoch': 0.66}\n",
      "{'loss': 0.0535, 'grad_norm': 0.13494396209716797, 'learning_rate': 5.265640409732522e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0834, 'grad_norm': 0.2352965623140335, 'learning_rate': 5.1607955999287446e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0524, 'grad_norm': 0.16865086555480957, 'learning_rate': 5.0566408597072447e-05, 'epoch': 0.67}\n",
      "{'loss': 0.0842, 'grad_norm': 0.16224460303783417, 'learning_rate': 4.95319104151321e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0601, 'grad_norm': 0.14712543785572052, 'learning_rate': 4.850460897270079e-05, 'epoch': 0.68}\n",
      "{'loss': 0.0582, 'grad_norm': 0.24948270618915558, 'learning_rate': 4.748465076275931e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0648, 'grad_norm': 0.10940279066562653, 'learning_rate': 4.647218123114494e-05, 'epoch': 0.69}\n",
      "{'loss': 0.0714, 'grad_norm': 0.16959337890148163, 'learning_rate': 4.546734475581103e-05, 'epoch': 0.69}\n",
      "{'loss': 0.057, 'grad_norm': 0.11734150350093842, 'learning_rate': 4.447028462623849e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0623, 'grad_norm': 0.1449775993824005, 'learning_rate': 4.348114302300291e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0578, 'grad_norm': 0.15038058161735535, 'learning_rate': 4.2500060997499725e-05, 'epoch': 0.7}\n",
      "{'loss': 0.0725, 'grad_norm': 0.21264350414276123, 'learning_rate': 4.152717845183e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0587, 'grad_norm': 0.11610535532236099, 'learning_rate': 4.056263411885081e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0756, 'grad_norm': 0.2729529142379761, 'learning_rate': 3.960656554239153e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0672, 'grad_norm': 0.14535823464393616, 'learning_rate': 3.865910905764059e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0716, 'grad_norm': 0.24478623270988464, 'learning_rate': 3.772039977170366e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0585, 'grad_norm': 0.15316015481948853, 'learning_rate': 3.6790571544337816e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0776, 'grad_norm': 0.20738759636878967, 'learning_rate': 3.586975696886268e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0654, 'grad_norm': 0.10214251279830933, 'learning_rate': 3.495808735325312e-05, 'epoch': 0.73}\n",
      "{'loss': 0.0556, 'grad_norm': 0.07856021076440811, 'learning_rate': 3.405569270141453e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0657, 'grad_norm': 0.2333582192659378, 'learning_rate': 3.316270169464425e-05, 'epoch': 0.74}\n",
      "{'loss': 0.059, 'grad_norm': 0.15961968898773193, 'learning_rate': 3.227924167328175e-05, 'epoch': 0.74}\n",
      "{'loss': 0.0644, 'grad_norm': 0.09213369339704514, 'learning_rate': 3.140543861854982e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0545, 'grad_norm': 0.08211267739534378, 'learning_rate': 3.0541417134589854e-05, 'epoch': 0.75}\n",
      "{'loss': 0.0559, 'grad_norm': 0.17549628019332886, 'learning_rate': 2.9687300430693122e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0649, 'grad_norm': 0.3269991874694824, 'learning_rate': 2.884321030373123e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0556, 'grad_norm': 0.18426795303821564, 'learning_rate': 2.8009267120787907e-05, 'epoch': 0.76}\n",
      "{'loss': 0.0671, 'grad_norm': 0.2523151636123657, 'learning_rate': 2.7185589801994748e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0557, 'grad_norm': 0.08218314498662949, 'learning_rate': 2.637229580357319e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0663, 'grad_norm': 0.2508983314037323, 'learning_rate': 2.5569501101085057e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0492, 'grad_norm': 0.12869185209274292, 'learning_rate': 2.4777320172894934e-05, 'epoch': 0.78}\n",
      "{'loss': 0.0637, 'grad_norm': 0.1551060825586319, 'learning_rate': 2.3995865983845088e-05, 'epoch': 0.78}\n",
      "{'loss': 0.053, 'grad_norm': 0.23609180748462677, 'learning_rate': 2.3225249969147132e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0688, 'grad_norm': 0.23732778429985046, 'learning_rate': 2.2465582018490937e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0534, 'grad_norm': 0.06985881179571152, 'learning_rate': 2.171697046037484e-05, 'epoch': 0.79}\n",
      "{'loss': 0.0562, 'grad_norm': 0.14852522313594818, 'learning_rate': 2.0979522046657653e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0535, 'grad_norm': 0.17573125660419464, 'learning_rate': 2.0253341937336157e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0618, 'grad_norm': 0.2241707146167755, 'learning_rate': 1.9538533685549077e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0556, 'grad_norm': 0.17739900946617126, 'learning_rate': 1.883519922281053e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0707, 'grad_norm': 0.308419793844223, 'learning_rate': 1.8143438844474657e-05, 'epoch': 0.81}\n",
      "{'loss': 0.045, 'grad_norm': 0.10245264321565628, 'learning_rate': 1.746335119543343e-05, 'epoch': 0.81}\n",
      "{'loss': 0.0632, 'grad_norm': 0.17403757572174072, 'learning_rate': 1.6795033256049953e-05, 'epoch': 0.82}\n",
      "{'loss': 0.0481, 'grad_norm': 0.08401764929294586, 'learning_rate': 1.6138580328329056e-05, 'epoch': 0.82}\n",
      "{'loss': 0.053, 'grad_norm': 0.3994951844215393, 'learning_rate': 1.549408602232737e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0466, 'grad_norm': 0.08833376318216324, 'learning_rate': 1.486164224280434e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0745, 'grad_norm': 0.1164180263876915, 'learning_rate': 1.4241339176116797e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0526, 'grad_norm': 0.14188972115516663, 'learning_rate': 1.3633265277358343e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0539, 'grad_norm': 0.16078336536884308, 'learning_rate': 1.303750725774563e-05, 'epoch': 0.84}\n",
      "{'loss': 0.0732, 'grad_norm': 0.06781748682260513, 'learning_rate': 1.2454150072253511e-05, 'epoch': 0.84}\n",
      "{'loss': 0.057, 'grad_norm': 0.1292002946138382, 'learning_rate': 1.1883276907500184e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0505, 'grad_norm': 0.09675871580839157, 'learning_rate': 1.1324969169885147e-05, 'epoch': 0.85}\n",
      "{'loss': 0.0609, 'grad_norm': 0.21955905854701996, 'learning_rate': 1.07793064739804e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0518, 'grad_norm': 0.09124094247817993, 'learning_rate': 1.0246366631177584e-05, 'epoch': 0.86}\n",
      "{'loss': 0.0486, 'grad_norm': 0.11073455214500427, 'learning_rate': 9.726225638591968e-06, 'epoch': 0.86}\n",
      "{'loss': 0.0665, 'grad_norm': 0.06224045902490616, 'learning_rate': 9.218957668225336e-06, 'epoch': 0.87}\n",
      "{'loss': 0.0697, 'grad_norm': 0.16049505770206451, 'learning_rate': 8.724635056389118e-06, 'epoch': 0.87}\n",
      "{'loss': 0.0627, 'grad_norm': 0.13109858334064484, 'learning_rate': 8.243328293389163e-06, 'epoch': 0.87}\n",
      "{'loss': 0.075, 'grad_norm': 0.3961014747619629, 'learning_rate': 7.775106013473876e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0576, 'grad_norm': 0.1349671334028244, 'learning_rate': 7.320034985046964e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0523, 'grad_norm': 0.14331792294979095, 'learning_rate': 6.878180101146348e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0474, 'grad_norm': 0.030230378732085228, 'learning_rate': 6.449604370190343e-06, 'epoch': 0.89}\n",
      "{'loss': 0.0528, 'grad_norm': 0.13395236432552338, 'learning_rate': 6.034368906992705e-06, 'epoch': 0.89}\n",
      "{'loss': 0.0497, 'grad_norm': 0.13887962698936462, 'learning_rate': 5.632532924047629e-06, 'epoch': 0.9}\n",
      "{'loss': 0.0628, 'grad_norm': 0.14462798833847046, 'learning_rate': 5.244153723086176e-06, 'epoch': 0.9}\n",
      "{'loss': 0.0557, 'grad_norm': 0.15708906948566437, 'learning_rate': 4.869286686904973e-06, 'epoch': 0.9}\n",
      "{'loss': 0.0589, 'grad_norm': 0.10900156199932098, 'learning_rate': 4.507985271468529e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0575, 'grad_norm': 0.1861010491847992, 'learning_rate': 4.160300998286692e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0649, 'grad_norm': 0.18455244600772858, 'learning_rate': 3.826283447067447e-06, 'epoch': 0.91}\n",
      "{'loss': 0.0488, 'grad_norm': 0.11195672303438187, 'learning_rate': 3.5059802486469785e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0574, 'grad_norm': 0.1721457839012146, 'learning_rate': 3.1994370781975093e-06, 'epoch': 0.92}\n",
      "{'loss': 0.0575, 'grad_norm': 0.06805462390184402, 'learning_rate': 2.9066976487139963e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0487, 'grad_norm': 0.24033188819885254, 'learning_rate': 2.627803704780696e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0478, 'grad_norm': 0.11583375185728073, 'learning_rate': 2.362795016618391e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0655, 'grad_norm': 0.13271035254001617, 'learning_rate': 2.1117093744131623e-06, 'epoch': 0.94}\n",
      "{'loss': 0.0591, 'grad_norm': 0.1593637615442276, 'learning_rate': 1.8745825829274866e-06, 'epoch': 0.94}\n",
      "{'loss': 0.0675, 'grad_norm': 0.10225651413202286, 'learning_rate': 1.6514484563945776e-06, 'epoch': 0.94}\n",
      "{'loss': 0.064, 'grad_norm': 0.15805290639400482, 'learning_rate': 1.4423388136963422e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0603, 'grad_norm': 0.12110165506601334, 'learning_rate': 1.247283473826144e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0648, 'grad_norm': 0.1470710039138794, 'learning_rate': 1.0663102516365153e-06, 'epoch': 0.95}\n",
      "{'loss': 0.0673, 'grad_norm': 0.12219517678022385, 'learning_rate': 8.994449538728744e-07, 'epoch': 0.96}\n",
      "{'loss': 0.0427, 'grad_norm': 0.08433417975902557, 'learning_rate': 7.467113754933919e-07, 'epoch': 0.96}\n",
      "{'loss': 0.0534, 'grad_norm': 0.24753595888614655, 'learning_rate': 6.081312962758712e-07, 'epoch': 0.97}\n",
      "{'loss': 0.0658, 'grad_norm': 0.13676239550113678, 'learning_rate': 4.837244777119887e-07, 'epoch': 0.97}\n",
      "{'loss': 0.0611, 'grad_norm': 0.30025628209114075, 'learning_rate': 3.735086601892368e-07, 'epoch': 0.97}\n",
      "{'loss': 0.0367, 'grad_norm': 0.027100343257188797, 'learning_rate': 2.7749956046123626e-07, 'epoch': 0.98}\n",
      "{'loss': 0.0596, 'grad_norm': 0.2376399040222168, 'learning_rate': 1.9571086940645133e-07, 'epoch': 0.98}\n",
      "{'loss': 0.0561, 'grad_norm': 0.12277793139219284, 'learning_rate': 1.2815425007589588e-07, 'epoch': 0.98}\n",
      "{'loss': 0.0687, 'grad_norm': 0.13482485711574554, 'learning_rate': 7.483933603004145e-08, 'epoch': 0.99}\n",
      "{'loss': 0.0501, 'grad_norm': 0.21875686943531036, 'learning_rate': 3.577372996497186e-08, 'epoch': 0.99}\n",
      "{'loss': 0.0535, 'grad_norm': 0.10867700725793839, 'learning_rate': 1.096300262833916e-08, 'epoch': 1.0}\n",
      "{'loss': 0.0418, 'grad_norm': 0.14621686935424805, 'learning_rate': 4.1069202489918414e-10, 'epoch': 1.0}\n",
      "{'train_runtime': 36007.3522, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.188, 'train_loss': 0.1218383721045874, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6781, training_loss=0.1218383721045874, metrics={'train_runtime': 36007.3522, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.188, 'total_flos': 9.20533750382592e+17, 'train_loss': 0.1218383721045874, 'epoch': 0.9999016957483411})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('StructLM-7B-luis_tokenizer/tokenizer_config.json',\n",
       " 'StructLM-7B-luis_tokenizer/special_tokens_map.json',\n",
       " 'StructLM-7B-luis_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model + \"_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
