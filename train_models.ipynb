{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "dataset_name = \"kokujin/prompts_1\"\n",
    "\n",
    "new_model = \"Llama-2-7b-hf-luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 8-bit precision base model loading\n",
    "use_8bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_8bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = f\"./results/{model_name}/\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 2\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 1\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 3\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = 1000\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea71f35f3514700b4b8d2d86525e0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a5b235603b426a9f13186e6dec8a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbee3e8e831e467e9b3ed2938f5aa319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3155d75a0a3e47718a98b363c20dadd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9793b4414e0349508b010eb86095100c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e2ffb094684a7e97b0f5669948ed0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f1b4adb6404535a41d237f4d0dab32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea8f8b0550a418b8d9e9310c2e2cf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49761018433d4bdfb49e0db30cabdc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9a2d9b2dff490fba9d3e542686cfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e679617f60644562bb8ca6349da18fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset (you can process it here)\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_8bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_8bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_8bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbd8915d34946439483edbab0348df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"Text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16d44491f194f55adb8b092401703f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0168, 'grad_norm': 0.07372236251831055, 'learning_rate': 2.604166666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0042, 'grad_norm': 0.20700715482234955, 'learning_rate': 5.208333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 0.562, 'grad_norm': 0.19536544382572174, 'learning_rate': 7.8125e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3159, 'grad_norm': 0.5842965841293335, 'learning_rate': 0.00010416666666666667, 'epoch': 0.03}\n",
      "{'loss': 0.2085, 'grad_norm': 0.1421627402305603, 'learning_rate': 0.00013020833333333333, 'epoch': 0.04}\n",
      "{'loss': 0.1915, 'grad_norm': 0.3209313750267029, 'learning_rate': 0.00015625, 'epoch': 0.05}\n",
      "{'loss': 0.1508, 'grad_norm': 0.20539532601833344, 'learning_rate': 0.00018229166666666667, 'epoch': 0.05}\n",
      "{'loss': 0.192, 'grad_norm': 0.44632020592689514, 'learning_rate': 0.00019999917944905216, 'epoch': 0.06}\n",
      "{'loss': 0.1521, 'grad_norm': 0.11595966666936874, 'learning_rate': 0.00019998603811858571, 'epoch': 0.07}\n",
      "{'loss': 0.1838, 'grad_norm': 0.17256402969360352, 'learning_rate': 0.00019995687283209658, 'epoch': 0.08}\n",
      "{'loss': 0.1369, 'grad_norm': 0.09807442128658295, 'learning_rate': 0.00019991168826367011, 'epoch': 0.09}\n",
      "{'loss': 0.1522, 'grad_norm': 0.2529987096786499, 'learning_rate': 0.00019985049165467257, 'epoch': 0.09}\n",
      "{'loss': 0.1232, 'grad_norm': 0.12034878879785538, 'learning_rate': 0.00019977329281259108, 'epoch': 0.1}\n",
      "{'loss': 0.1607, 'grad_norm': 0.23607578873634338, 'learning_rate': 0.00019968010410946154, 'epoch': 0.11}\n",
      "{'loss': 0.1228, 'grad_norm': 0.12443778663873672, 'learning_rate': 0.00019957094047988582, 'epoch': 0.12}\n",
      "{'loss': 0.1352, 'grad_norm': 0.17415718734264374, 'learning_rate': 0.00019944581941863857, 'epoch': 0.13}\n",
      "{'loss': 0.1161, 'grad_norm': 0.1217331513762474, 'learning_rate': 0.00019930476097786327, 'epoch': 0.13}\n",
      "{'loss': 0.1501, 'grad_norm': 0.3952060341835022, 'learning_rate': 0.0001991477877638587, 'epoch': 0.14}\n",
      "{'loss': 0.1235, 'grad_norm': 0.14330002665519714, 'learning_rate': 0.00019897492493345598, 'epoch': 0.15}\n",
      "{'loss': 0.1307, 'grad_norm': 0.11016341298818588, 'learning_rate': 0.00019878620018998696, 'epoch': 0.16}\n",
      "{'loss': 0.111, 'grad_norm': 0.10635470598936081, 'learning_rate': 0.00019858164377884436, 'epoch': 0.16}\n",
      "{'loss': 0.1326, 'grad_norm': 0.11806108802556992, 'learning_rate': 0.00019836128848263465, 'epoch': 0.17}\n",
      "{'loss': 0.1028, 'grad_norm': 0.09999821335077286, 'learning_rate': 0.0001981251696159241, 'epoch': 0.18}\n",
      "{'loss': 0.1288, 'grad_norm': 0.14281988143920898, 'learning_rate': 0.00019787332501957941, 'epoch': 0.19}\n",
      "{'loss': 0.1119, 'grad_norm': 0.11818540841341019, 'learning_rate': 0.0001976057950547031, 'epoch': 0.2}\n",
      "{'loss': 0.1374, 'grad_norm': 0.10373816639184952, 'learning_rate': 0.00019732262259616523, 'epoch': 0.2}\n",
      "{'loss': 0.0974, 'grad_norm': 0.13401280343532562, 'learning_rate': 0.0001970238530257322, 'epoch': 0.21}\n",
      "{'loss': 0.1041, 'grad_norm': 0.09350785613059998, 'learning_rate': 0.00019670953422479368, 'epoch': 0.22}\n",
      "{'loss': 0.0886, 'grad_norm': 0.09239175170660019, 'learning_rate': 0.00019637971656668918, 'epoch': 0.23}\n",
      "{'loss': 0.1213, 'grad_norm': 0.2101534754037857, 'learning_rate': 0.0001960344529086351, 'epoch': 0.23}\n",
      "{'loss': 0.1081, 'grad_norm': 0.12314482778310776, 'learning_rate': 0.00019567379858325356, 'epoch': 0.24}\n",
      "{'loss': 0.1094, 'grad_norm': 0.2207559049129486, 'learning_rate': 0.000195297811389705, 'epoch': 0.25}\n",
      "{'loss': 0.0986, 'grad_norm': 0.09047196060419083, 'learning_rate': 0.00019490655158442484, 'epoch': 0.26}\n",
      "{'loss': 0.1142, 'grad_norm': 0.12040792405605316, 'learning_rate': 0.00019450008187146684, 'epoch': 0.27}\n",
      "{'loss': 0.0994, 'grad_norm': 0.08893821388483047, 'learning_rate': 0.00019407846739245415, 'epoch': 0.27}\n",
      "{'loss': 0.1136, 'grad_norm': 0.11084236204624176, 'learning_rate': 0.00019364177571613926, 'epoch': 0.28}\n",
      "{'loss': 0.097, 'grad_norm': 0.13034236431121826, 'learning_rate': 0.00019319007682757556, 'epoch': 0.29}\n",
      "{'loss': 0.0933, 'grad_norm': 0.1325632780790329, 'learning_rate': 0.0001927234431169014, 'epoch': 0.3}\n",
      "{'loss': 0.0756, 'grad_norm': 0.1000635102391243, 'learning_rate': 0.00019224194936773853, 'epoch': 0.3}\n",
      "{'loss': 0.0923, 'grad_norm': 0.22215475142002106, 'learning_rate': 0.00019174567274520728, 'epoch': 0.31}\n",
      "{'loss': 0.0739, 'grad_norm': 0.125279501080513, 'learning_rate': 0.00019123469278355985, 'epoch': 0.32}\n",
      "{'loss': 0.0919, 'grad_norm': 0.19598177075386047, 'learning_rate': 0.00019070909137343408, 'epoch': 0.33}\n",
      "{'loss': 0.074, 'grad_norm': 0.1032618060708046, 'learning_rate': 0.0001901689527487294, 'epoch': 0.34}\n",
      "{'loss': 0.0913, 'grad_norm': 0.10973244160413742, 'learning_rate': 0.0001896143634731074, 'epoch': 0.34}\n",
      "{'loss': 0.0829, 'grad_norm': 0.10300366580486298, 'learning_rate': 0.00018904541242611902, 'epoch': 0.35}\n",
      "{'loss': 0.1052, 'grad_norm': 0.15103451907634735, 'learning_rate': 0.00018846219078896037, 'epoch': 0.36}\n",
      "{'loss': 0.0842, 'grad_norm': 0.09712079912424088, 'learning_rate': 0.00018786479202986006, 'epoch': 0.37}\n",
      "{'loss': 0.0976, 'grad_norm': 0.14048095047473907, 'learning_rate': 0.0001872533118890997, 'epoch': 0.38}\n",
      "{'loss': 0.0767, 'grad_norm': 0.10973868519067764, 'learning_rate': 0.00018662784836367028, 'epoch': 0.38}\n",
      "{'loss': 0.0878, 'grad_norm': 0.13113722205162048, 'learning_rate': 0.00018598850169156722, 'epoch': 0.39}\n",
      "{'loss': 0.0796, 'grad_norm': 0.09953539073467255, 'learning_rate': 0.00018533537433572581, 'epoch': 0.4}\n",
      "{'loss': 0.0862, 'grad_norm': 0.15510617196559906, 'learning_rate': 0.00018466857096760046, 'epoch': 0.41}\n",
      "{'loss': 0.0759, 'grad_norm': 0.07445394992828369, 'learning_rate': 0.00018398819845038972, 'epoch': 0.41}\n",
      "{'loss': 0.0925, 'grad_norm': 0.247817263007164, 'learning_rate': 0.0001832943658219103, 'epoch': 0.42}\n",
      "{'loss': 0.0732, 'grad_norm': 0.11674395948648453, 'learning_rate': 0.00018258718427712238, 'epoch': 0.43}\n",
      "{'loss': 0.0865, 'grad_norm': 0.14407627284526825, 'learning_rate': 0.00018186676715030924, 'epoch': 0.44}\n",
      "{'loss': 0.0748, 'grad_norm': 0.08160194009542465, 'learning_rate': 0.0001811332298969142, 'epoch': 0.45}\n",
      "{'loss': 0.0981, 'grad_norm': 0.14886535704135895, 'learning_rate': 0.0001803866900750376, 'epoch': 0.45}\n",
      "{'loss': 0.0717, 'grad_norm': 0.09281537681818008, 'learning_rate': 0.0001796272673265963, 'epoch': 0.46}\n",
      "{'loss': 0.0792, 'grad_norm': 0.09734966605901718, 'learning_rate': 0.00017885508335815014, 'epoch': 0.47}\n",
      "{'loss': 0.0679, 'grad_norm': 0.10651493072509766, 'learning_rate': 0.0001780702619213967, 'epoch': 0.48}\n",
      "{'loss': 0.0817, 'grad_norm': 0.19400672614574432, 'learning_rate': 0.0001772729287933387, 'epoch': 0.48}\n",
      "{'loss': 0.0701, 'grad_norm': 0.10392797738313675, 'learning_rate': 0.00017646321175612668, 'epoch': 0.49}\n",
      "{'loss': 0.0874, 'grad_norm': 0.17100906372070312, 'learning_rate': 0.00017564124057658056, 'epoch': 0.5}\n",
      "{'loss': 0.0655, 'grad_norm': 0.14439144730567932, 'learning_rate': 0.00017480714698539266, 'epoch': 0.51}\n",
      "{'loss': 0.0772, 'grad_norm': 0.09884030371904373, 'learning_rate': 0.00017396106465601663, 'epoch': 0.52}\n",
      "{'loss': 0.0736, 'grad_norm': 0.07561030238866806, 'learning_rate': 0.0001731031291832444, 'epoch': 0.52}\n",
      "{'loss': 0.0824, 'grad_norm': 0.14098410308361053, 'learning_rate': 0.0001722334780614756, 'epoch': 0.53}\n",
      "{'loss': 0.0731, 'grad_norm': 0.09904792904853821, 'learning_rate': 0.00017135225066268255, 'epoch': 0.54}\n",
      "{'loss': 0.0811, 'grad_norm': 0.21149154007434845, 'learning_rate': 0.00017045958821407405, 'epoch': 0.55}\n",
      "{'loss': 0.0708, 'grad_norm': 0.08569848537445068, 'learning_rate': 0.00016955563377546207, 'epoch': 0.55}\n",
      "{'loss': 0.0802, 'grad_norm': 0.1469348818063736, 'learning_rate': 0.0001686405322163349, 'epoch': 0.56}\n",
      "{'loss': 0.0636, 'grad_norm': 0.10753300040960312, 'learning_rate': 0.00016771443019263983, 'epoch': 0.57}\n",
      "{'loss': 0.0729, 'grad_norm': 0.15582260489463806, 'learning_rate': 0.00016677747612327997, 'epoch': 0.58}\n",
      "{'loss': 0.0646, 'grad_norm': 0.11182352900505066, 'learning_rate': 0.00016582982016632818, 'epoch': 0.59}\n",
      "{'loss': 0.0685, 'grad_norm': 0.10339730232954025, 'learning_rate': 0.00016487161419496263, 'epoch': 0.59}\n",
      "{'loss': 0.0571, 'grad_norm': 0.13226759433746338, 'learning_rate': 0.00016390301177312722, 'epoch': 0.6}\n",
      "{'loss': 0.0801, 'grad_norm': 0.14834819734096527, 'learning_rate': 0.00016292416813092105, 'epoch': 0.61}\n",
      "{'loss': 0.0648, 'grad_norm': 0.09808413684368134, 'learning_rate': 0.00016193524013972114, 'epoch': 0.62}\n",
      "{'loss': 0.0843, 'grad_norm': 0.15677288174629211, 'learning_rate': 0.00016093638628704167, 'epoch': 0.63}\n",
      "{'loss': 0.0514, 'grad_norm': 0.0766884908080101, 'learning_rate': 0.0001599277666511347, 'epoch': 0.63}\n",
      "{'loss': 0.0767, 'grad_norm': 0.20364819467067719, 'learning_rate': 0.00015890954287533555, 'epoch': 0.64}\n",
      "{'loss': 0.0568, 'grad_norm': 0.12740683555603027, 'learning_rate': 0.00015788187814215764, 'epoch': 0.65}\n",
      "{'loss': 0.076, 'grad_norm': 0.12899045646190643, 'learning_rate': 0.00015684493714714047, 'epoch': 0.66}\n",
      "{'loss': 0.0509, 'grad_norm': 0.1087249293923378, 'learning_rate': 0.00015579888607245517, 'epoch': 0.66}\n",
      "{'loss': 0.078, 'grad_norm': 0.14588384330272675, 'learning_rate': 0.000154743892560272, 'epoch': 0.67}\n",
      "{'loss': 0.0506, 'grad_norm': 0.08713212609291077, 'learning_rate': 0.00015368012568589342, 'epoch': 0.68}\n",
      "{'loss': 0.0713, 'grad_norm': 0.1444116085767746, 'learning_rate': 0.00015260775593065802, 'epoch': 0.69}\n",
      "{'loss': 0.0571, 'grad_norm': 0.08425400406122208, 'learning_rate': 0.00015152695515461865, 'epoch': 0.7}\n",
      "{'loss': 0.071, 'grad_norm': 0.12301599234342575, 'learning_rate': 0.00015043789656899988, 'epoch': 0.7}\n",
      "{'loss': 0.0466, 'grad_norm': 0.15080828964710236, 'learning_rate': 0.00014934075470843887, 'epoch': 0.71}\n",
      "{'loss': 0.0717, 'grad_norm': 0.15468105673789978, 'learning_rate': 0.00014823570540301408, 'epoch': 0.72}\n",
      "{'loss': 0.053, 'grad_norm': 0.09826921671628952, 'learning_rate': 0.00014712292575006633, 'epoch': 0.73}\n",
      "{'loss': 0.084, 'grad_norm': 0.2159779816865921, 'learning_rate': 0.00014600259408581687, 'epoch': 0.73}\n",
      "{'loss': 0.0489, 'grad_norm': 0.08869534730911255, 'learning_rate': 0.00014487488995678708, 'epoch': 0.74}\n",
      "{'loss': 0.0568, 'grad_norm': 0.13357573747634888, 'learning_rate': 0.00014373999409102362, 'epoch': 0.75}\n",
      "{'loss': 0.0526, 'grad_norm': 0.07367324084043503, 'learning_rate': 0.00014259808836913492, 'epoch': 0.76}\n",
      "{'loss': 0.0809, 'grad_norm': 0.15547014772891998, 'learning_rate': 0.00014144935579514246, 'epoch': 0.77}\n",
      "{'loss': 0.0513, 'grad_norm': 0.12713147699832916, 'learning_rate': 0.00014029398046715223, 'epoch': 0.77}\n",
      "{'loss': 0.0559, 'grad_norm': 0.11993339657783508, 'learning_rate': 0.00013913214754785095, 'epoch': 0.78}\n",
      "{'loss': 0.0521, 'grad_norm': 0.09020290523767471, 'learning_rate': 0.00013796404323483132, 'epoch': 0.79}\n",
      "{'loss': 0.0541, 'grad_norm': 0.08147749304771423, 'learning_rate': 0.00013678985473075176, 'epoch': 0.8}\n",
      "{'loss': 0.0498, 'grad_norm': 0.07862571626901627, 'learning_rate': 0.00013560977021333497, 'epoch': 0.81}\n",
      "{'loss': 0.0748, 'grad_norm': 0.19405068457126617, 'learning_rate': 0.00013442397880521008, 'epoch': 0.81}\n",
      "{'loss': 0.0538, 'grad_norm': 0.11760378628969193, 'learning_rate': 0.0001332326705436037, 'epoch': 0.82}\n",
      "{'loss': 0.0618, 'grad_norm': 0.12941712141036987, 'learning_rate': 0.00013203603634988386, 'epoch': 0.83}\n",
      "{'loss': 0.0564, 'grad_norm': 0.10309737920761108, 'learning_rate': 0.000130834267998963, 'epoch': 0.84}\n",
      "{'loss': 0.0593, 'grad_norm': 0.22142913937568665, 'learning_rate': 0.00012962755808856342, 'epoch': 0.84}\n",
      "{'loss': 0.0439, 'grad_norm': 0.08934997767210007, 'learning_rate': 0.00012841610000835125, 'epoch': 0.85}\n",
      "{'loss': 0.0663, 'grad_norm': 0.17143645882606506, 'learning_rate': 0.00012720008790894366, 'epoch': 0.86}\n",
      "{'loss': 0.0399, 'grad_norm': 0.13781222701072693, 'learning_rate': 0.00012597971667079361, 'epoch': 0.87}\n",
      "{'loss': 0.0672, 'grad_norm': 0.16726554930210114, 'learning_rate': 0.0001247551818729582, 'epoch': 0.88}\n",
      "{'loss': 0.0437, 'grad_norm': 0.1105312705039978, 'learning_rate': 0.0001235266797617545, 'epoch': 0.88}\n",
      "{'loss': 0.0566, 'grad_norm': 0.11321337521076202, 'learning_rate': 0.00012229440721930906, 'epoch': 0.89}\n",
      "{'loss': 0.047, 'grad_norm': 0.12500178813934326, 'learning_rate': 0.00012105856173200498, 'epoch': 0.9}\n",
      "{'loss': 0.0607, 'grad_norm': 0.13301007449626923, 'learning_rate': 0.00011981934135883237, 'epoch': 0.91}\n",
      "{'loss': 0.0532, 'grad_norm': 0.10338349640369415, 'learning_rate': 0.00011857694469964715, 'epoch': 0.91}\n",
      "{'loss': 0.057, 'grad_norm': 0.14267335832118988, 'learning_rate': 0.00011733157086334294, 'epoch': 0.92}\n",
      "{'loss': 0.0459, 'grad_norm': 0.1078609824180603, 'learning_rate': 0.0001160834194359416, 'epoch': 0.93}\n",
      "{'loss': 0.0766, 'grad_norm': 0.1461758017539978, 'learning_rate': 0.00011483269044860705, 'epoch': 0.94}\n",
      "{'loss': 0.0429, 'grad_norm': 0.12601222097873688, 'learning_rate': 0.000113579584345588, 'epoch': 0.95}\n",
      "{'loss': 0.0665, 'grad_norm': 0.10900402814149857, 'learning_rate': 0.0001123243019520942, 'epoch': 0.95}\n",
      "{'loss': 0.0425, 'grad_norm': 0.12857025861740112, 'learning_rate': 0.00011106704444211207, 'epoch': 0.96}\n",
      "{'loss': 0.053, 'grad_norm': 0.1640956550836563, 'learning_rate': 0.000109808013306164, 'epoch': 0.97}\n",
      "{'loss': 0.043, 'grad_norm': 0.11999285221099854, 'learning_rate': 0.00010854741031901703, 'epoch': 0.98}\n",
      "{'loss': 0.0738, 'grad_norm': 0.14193956553936005, 'learning_rate': 0.00010728543750734622, 'epoch': 0.98}\n",
      "{'loss': 0.037, 'grad_norm': 0.09917745739221573, 'learning_rate': 0.00010602229711735726, 'epoch': 0.99}\n",
      "{'loss': 0.0597, 'grad_norm': 0.07736478000879288, 'learning_rate': 0.00010475819158237425, 'epoch': 1.0}\n",
      "{'loss': 0.0338, 'grad_norm': 0.08551782369613647, 'learning_rate': 0.0001034933234903973, 'epoch': 1.01}\n",
      "{'loss': 0.0571, 'grad_norm': 0.1320209801197052, 'learning_rate': 0.0001022278955516354, 'epoch': 1.02}\n",
      "{'loss': 0.0452, 'grad_norm': 0.07304678112268448, 'learning_rate': 0.00010096211056601958, 'epoch': 1.02}\n",
      "{'loss': 0.0534, 'grad_norm': 0.1066020131111145, 'learning_rate': 9.969617139070202e-05, 'epoch': 1.03}\n",
      "{'loss': 0.0367, 'grad_norm': 0.14709720015525818, 'learning_rate': 9.84302809075455e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0551, 'grad_norm': 0.10040151327848434, 'learning_rate': 9.716464199060946e-05, 'epoch': 1.05}\n",
      "{'loss': 0.041, 'grad_norm': 0.10246758162975311, 'learning_rate': 9.589945747363667e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0536, 'grad_norm': 0.09503686428070068, 'learning_rate': 9.463493011754706e-05, 'epoch': 1.06}\n",
      "{'loss': 0.0306, 'grad_norm': 0.09478604793548584, 'learning_rate': 9.337126257794255e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0411, 'grad_norm': 0.12112786620855331, 'learning_rate': 9.210865737262924e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0405, 'grad_norm': 0.17108869552612305, 'learning_rate': 9.084731684916151e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0469, 'grad_norm': 0.04287311062216759, 'learning_rate': 8.958744315241341e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0392, 'grad_norm': 0.06616378575563431, 'learning_rate': 8.832923819218238e-05, 'epoch': 1.1}\n",
      "{'loss': 0.0429, 'grad_norm': 0.08639610558748245, 'learning_rate': 8.707290361083107e-05, 'epoch': 1.11}\n",
      "{'loss': 0.0373, 'grad_norm': 0.09512147307395935, 'learning_rate': 8.581864075097144e-05, 'epoch': 1.12}\n",
      "{'loss': 0.0384, 'grad_norm': 0.0561474934220314, 'learning_rate': 8.456665062319742e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0401, 'grad_norm': 0.06767480820417404, 'learning_rate': 8.33171338738706e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0522, 'grad_norm': 0.08025548607110977, 'learning_rate': 8.207029075296392e-05, 'epoch': 1.14}\n",
      "{'loss': 0.035, 'grad_norm': 0.06865951418876648, 'learning_rate': 8.082632108196969e-05, 'epoch': 1.15}\n",
      "{'loss': 0.0503, 'grad_norm': 0.08558903634548187, 'learning_rate': 7.958542422187538e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0324, 'grad_norm': 0.061024345457553864, 'learning_rate': 7.834779904121399e-05, 'epoch': 1.16}\n",
      "{'loss': 0.0493, 'grad_norm': 0.07680206745862961, 'learning_rate': 7.711364388419278e-05, 'epoch': 1.17}\n",
      "{'loss': 0.0326, 'grad_norm': 0.11576440930366516, 'learning_rate': 7.588315653890629e-05, 'epoch': 1.18}\n",
      "{'loss': 0.0385, 'grad_norm': 0.08361730724573135, 'learning_rate': 7.465653420563845e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0348, 'grad_norm': 0.10096333175897598, 'learning_rate': 7.343397346525888e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0479, 'grad_norm': 0.07992786169052124, 'learning_rate': 7.221567024771849e-05, 'epoch': 1.2}\n",
      "{'loss': 0.0332, 'grad_norm': 0.10847412049770355, 'learning_rate': 7.100181980064937e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0467, 'grad_norm': 0.14536519348621368, 'learning_rate': 6.979261665807389e-05, 'epoch': 1.22}\n",
      "{'loss': 0.036, 'grad_norm': 0.03416386991739273, 'learning_rate': 6.858825460922849e-05, 'epoch': 1.23}\n",
      "{'loss': 0.0433, 'grad_norm': 0.074139803647995, 'learning_rate': 6.738892666750651e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0269, 'grad_norm': 0.12245995551347733, 'learning_rate': 6.619482503952559e-05, 'epoch': 1.24}\n",
      "{'loss': 0.0453, 'grad_norm': 0.02719786763191223, 'learning_rate': 6.500614109432419e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0361, 'grad_norm': 0.07890438288450241, 'learning_rate': 6.382306533269238e-05, 'epoch': 1.26}\n",
      "{'loss': 0.0407, 'grad_norm': 0.17288175225257874, 'learning_rate': 6.264578735664194e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0297, 'grad_norm': 0.06969442963600159, 'learning_rate': 6.147449583902036e-05, 'epoch': 1.27}\n",
      "{'loss': 0.0374, 'grad_norm': 0.1062152162194252, 'learning_rate': 6.030937849327356e-05, 'epoch': 1.28}\n",
      "{'loss': 0.0349, 'grad_norm': 0.16284960508346558, 'learning_rate': 5.91506220433629e-05, 'epoch': 1.29}\n",
      "{'loss': 0.0504, 'grad_norm': 0.08115194737911224, 'learning_rate': 5.79984121938401e-05, 'epoch': 1.3}\n",
      "{'loss': 0.0352, 'grad_norm': 0.061517901718616486, 'learning_rate': 5.6852933600086125e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0396, 'grad_norm': 0.10849671065807343, 'learning_rate': 5.5714369838717874e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0448, 'grad_norm': 0.1651451587677002, 'learning_rate': 5.4582903378167716e-05, 'epoch': 1.32}\n",
      "{'loss': 0.0501, 'grad_norm': 0.08405693620443344, 'learning_rate': 5.3458715549440984e-05, 'epoch': 1.33}\n",
      "{'loss': 0.0362, 'grad_norm': 0.12146975100040436, 'learning_rate': 5.234198651705527e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0364, 'grad_norm': 0.05690561607480049, 'learning_rate': 5.12328952501671e-05, 'epoch': 1.34}\n",
      "{'loss': 0.0394, 'grad_norm': 0.16917158663272858, 'learning_rate': 5.013161949388993e-05, 'epoch': 1.35}\n",
      "{'loss': 0.0408, 'grad_norm': 0.08387745916843414, 'learning_rate': 4.903833574080825e-05, 'epoch': 1.36}\n",
      "{'loss': 0.0287, 'grad_norm': 0.04665641486644745, 'learning_rate': 4.795321920269279e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0396, 'grad_norm': 0.1157321110367775, 'learning_rate': 4.687644378242044e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0329, 'grad_norm': 0.06476232409477234, 'learning_rate': 4.580818204610458e-05, 'epoch': 1.38}\n",
      "{'loss': 0.0476, 'grad_norm': 0.06882144510746002, 'learning_rate': 4.4748605195438976e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0317, 'grad_norm': 0.08675823360681534, 'learning_rate': 4.36978830402608e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0494, 'grad_norm': 0.08124297112226486, 'learning_rate': 4.265618397133674e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0319, 'grad_norm': 0.1294747292995453, 'learning_rate': 4.162367493337601e-05, 'epoch': 1.41}\n",
      "{'loss': 0.0394, 'grad_norm': 0.062247391790151596, 'learning_rate': 4.060052139827582e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0261, 'grad_norm': 0.12889812886714935, 'learning_rate': 3.958688733860237e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0333, 'grad_norm': 0.11567224562168121, 'learning_rate': 3.858293520131221e-05, 'epoch': 1.44}\n",
      "{'loss': 0.0296, 'grad_norm': 0.1005009189248085, 'learning_rate': 3.758882588171837e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0366, 'grad_norm': 0.041975222527980804, 'learning_rate': 3.660471869770474e-05, 'epoch': 1.45}\n",
      "{'loss': 0.0281, 'grad_norm': 0.03893091157078743, 'learning_rate': 3.563077136419373e-05, 'epoch': 1.46}\n",
      "{'loss': 0.0434, 'grad_norm': 0.07120797038078308, 'learning_rate': 3.466713996787039e-05, 'epoch': 1.47}\n",
      "{'loss': 0.0314, 'grad_norm': 0.08539961278438568, 'learning_rate': 3.371397894216766e-05, 'epoch': 1.48}\n",
      "{'loss': 0.0343, 'grad_norm': 0.06168881058692932, 'learning_rate': 3.277144104251669e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0243, 'grad_norm': 0.11557033658027649, 'learning_rate': 3.183967732186582e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0467, 'grad_norm': 0.054865967482328415, 'learning_rate': 3.0918837106472686e-05, 'epoch': 1.5}\n",
      "{'loss': 0.0267, 'grad_norm': 0.03896065428853035, 'learning_rate': 3.0009067971972716e-05, 'epoch': 1.51}\n",
      "{'loss': 0.0489, 'grad_norm': 0.09216874092817307, 'learning_rate': 2.9110515719728594e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0261, 'grad_norm': 0.024320168420672417, 'learning_rate': 2.8223324353463644e-05, 'epoch': 1.52}\n",
      "{'loss': 0.0473, 'grad_norm': 0.05681990087032318, 'learning_rate': 2.73476360561837e-05, 'epoch': 1.53}\n",
      "{'loss': 0.0258, 'grad_norm': 0.06487660109996796, 'learning_rate': 2.6483591167390407e-05, 'epoch': 1.54}\n",
      "{'loss': 0.0426, 'grad_norm': 0.048495061695575714, 'learning_rate': 2.5631328160590318e-05, 'epoch': 1.55}\n",
      "{'loss': 0.0249, 'grad_norm': 0.0648014023900032, 'learning_rate': 2.479098362110267e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0236, 'grad_norm': 0.10654781758785248, 'learning_rate': 2.3962692224170114e-05, 'epoch': 1.56}\n",
      "{'loss': 0.0283, 'grad_norm': 0.024342849850654602, 'learning_rate': 2.3146586713375395e-05, 'epoch': 1.57}\n",
      "{'loss': 0.0478, 'grad_norm': 0.03248557075858116, 'learning_rate': 2.2342797879367418e-05, 'epoch': 1.58}\n",
      "{'loss': 0.0357, 'grad_norm': 0.07841094583272934, 'learning_rate': 2.155145453890076e-05, 'epoch': 1.59}\n",
      "{'loss': 0.046, 'grad_norm': 0.061154190450906754, 'learning_rate': 2.0772683514191004e-05, 'epoch': 1.59}\n",
      "{'loss': 0.0312, 'grad_norm': 0.02604280412197113, 'learning_rate': 2.0006609612590142e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0422, 'grad_norm': 0.154628187417984, 'learning_rate': 1.9253355606584655e-05, 'epoch': 1.61}\n",
      "{'loss': 0.0313, 'grad_norm': 0.07763708382844925, 'learning_rate': 1.851304221411967e-05, 'epoch': 1.62}\n",
      "{'loss': 0.0417, 'grad_norm': 0.024154657498002052, 'learning_rate': 1.778578807925253e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0331, 'grad_norm': 0.11182829737663269, 'learning_rate': 1.707170975313879e-05, 'epoch': 1.63}\n",
      "{'loss': 0.0355, 'grad_norm': 0.09835804998874664, 'learning_rate': 1.6370921675353223e-05, 'epoch': 1.64}\n",
      "{'loss': 0.0246, 'grad_norm': 0.12325156480073929, 'learning_rate': 1.568353615554985e-05, 'epoch': 1.65}\n",
      "{'loss': 0.0432, 'grad_norm': 0.03831395506858826, 'learning_rate': 1.5009663355462655e-05, 'epoch': 1.66}\n",
      "{'loss': 0.0231, 'grad_norm': 0.0989769920706749, 'learning_rate': 1.4349411271251134e-05, 'epoch': 1.66}\n",
      "{'loss': 0.027, 'grad_norm': 0.1014106422662735, 'learning_rate': 1.3702885716192348e-05, 'epoch': 1.67}\n",
      "{'loss': 0.0304, 'grad_norm': 0.017909711226820946, 'learning_rate': 1.3070190303723352e-05, 'epoch': 1.68}\n",
      "{'loss': 0.032, 'grad_norm': 0.05780309811234474, 'learning_rate': 1.2451426430835733e-05, 'epoch': 1.69}\n",
      "{'loss': 0.0243, 'grad_norm': 0.021492179483175278, 'learning_rate': 1.1846693261825525e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0482, 'grad_norm': 0.0988890752196312, 'learning_rate': 1.1256087712401087e-05, 'epoch': 1.7}\n",
      "{'loss': 0.0282, 'grad_norm': 0.07326465100049973, 'learning_rate': 1.0679704434151016e-05, 'epoch': 1.71}\n",
      "{'loss': 0.0334, 'grad_norm': 0.12024109810590744, 'learning_rate': 1.0117635799375291e-05, 'epoch': 1.72}\n",
      "{'loss': 0.0246, 'grad_norm': 0.14622581005096436, 'learning_rate': 9.569971886281392e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0346, 'grad_norm': 0.14746370911598206, 'learning_rate': 9.036800464548157e-06, 'epoch': 1.74}\n",
      "{'loss': 0.0297, 'grad_norm': 0.10352254658937454, 'learning_rate': 8.51820698125979e-06, 'epoch': 1.74}\n",
      "{'loss': 0.037, 'grad_norm': 0.13999219238758087, 'learning_rate': 8.014274547211808e-06, 'epoch': 1.75}\n",
      "{'loss': 0.031, 'grad_norm': 0.08001399040222168, 'learning_rate': 7.525083923591592e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0299, 'grad_norm': 0.033812686800956726, 'learning_rate': 7.050713509035478e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0268, 'grad_norm': 0.030413739383220673, 'learning_rate': 6.591239327064391e-06, 'epoch': 1.77}\n",
      "{'loss': 0.0395, 'grad_norm': 0.02061101235449314, 'learning_rate': 6.146735013900173e-06, 'epoch': 1.78}\n",
      "{'loss': 0.03, 'grad_norm': 0.07740870863199234, 'learning_rate': 5.717271806664559e-06, 'epoch': 1.79}\n",
      "{'loss': 0.0324, 'grad_norm': 0.02687220834195614, 'learning_rate': 5.302918531962464e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0223, 'grad_norm': 0.10927591472864151, 'learning_rate': 4.903741594851841e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0409, 'grad_norm': 0.1448247730731964, 'learning_rate': 4.519804968201313e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0245, 'grad_norm': 0.02746441215276718, 'learning_rate': 4.151170182437924e-06, 'epoch': 1.82}\n",
      "{'loss': 0.0465, 'grad_norm': 0.029349112883210182, 'learning_rate': 3.7978963156860446e-06, 'epoch': 1.83}\n",
      "{'loss': 0.0244, 'grad_norm': 0.07824627310037613, 'learning_rate': 3.4600399842994237e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0367, 'grad_norm': 0.1151382327079773, 'learning_rate': 3.13765533378777e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0228, 'grad_norm': 0.13133320212364197, 'learning_rate': 2.8307940301392164e-06, 'epoch': 1.85}\n",
      "{'loss': 0.041, 'grad_norm': 0.02269325591623783, 'learning_rate': 2.539505251540353e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0249, 'grad_norm': 0.06693825125694275, 'learning_rate': 2.263835680494686e-06, 'epoch': 1.87}\n",
      "{'loss': 0.0422, 'grad_norm': 0.11496422439813614, 'learning_rate': 2.003829496341325e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0259, 'grad_norm': 0.12322395294904709, 'learning_rate': 1.7595283681746678e-06, 'epoch': 1.88}\n",
      "{'loss': 0.0339, 'grad_norm': 0.12849709391593933, 'learning_rate': 1.5309714481664183e-06, 'epoch': 1.89}\n",
      "{'loss': 0.0271, 'grad_norm': 0.032151639461517334, 'learning_rate': 1.3181953652910305e-06, 'epoch': 1.9}\n",
      "{'loss': 0.0234, 'grad_norm': 0.08345422148704529, 'learning_rate': 1.121234219455447e-06, 'epoch': 1.91}\n",
      "{'loss': 0.028, 'grad_norm': 0.12398143857717514, 'learning_rate': 9.401195760341708e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0312, 'grad_norm': 0.043297987431287766, 'learning_rate': 7.748804608105675e-07, 'epoch': 1.92}\n",
      "{'loss': 0.0287, 'grad_norm': 0.06788438558578491, 'learning_rate': 6.255433553250978e-07, 'epoch': 1.93}\n",
      "{'loss': 0.0312, 'grad_norm': 0.07860343158245087, 'learning_rate': 4.921321926313893e-07, 'epoch': 1.94}\n",
      "{'loss': 0.0174, 'grad_norm': 0.027127010747790337, 'learning_rate': 3.746683534606277e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0368, 'grad_norm': 0.14055302739143372, 'learning_rate': 2.7317066279506363e-07, 'epoch': 1.95}\n",
      "{'loss': 0.0309, 'grad_norm': 0.06083441898226738, 'learning_rate': 1.8765538685108218e-07, 'epoch': 1.96}\n",
      "{'loss': 0.0406, 'grad_norm': 0.10796944051980972, 'learning_rate': 1.1813623047236544e-07, 'epoch': 1.97}\n",
      "{'loss': 0.0257, 'grad_norm': 0.06745754182338715, 'learning_rate': 6.462433493347187e-08, 'epoch': 1.98}\n",
      "{'loss': 0.0369, 'grad_norm': 0.02873641438782215, 'learning_rate': 2.712827615437563e-08, 'epoch': 1.99}\n",
      "{'loss': 0.0342, 'grad_norm': 0.10258662700653076, 'learning_rate': 5.654063326032688e-09, 'epoch': 1.99}\n",
      "{'train_runtime': 29044.4096, 'train_samples_per_second': 0.661, 'train_steps_per_second': 0.22, 'train_loss': 0.07030785922485265, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6396, training_loss=0.07030785922485265, metrics={'train_runtime': 29044.4096, 'train_samples_per_second': 0.661, 'train_steps_per_second': 0.22, 'total_flos': 7.535116752308797e+17, 'train_loss': 0.07030785922485265, 'epoch': 1.9997915581031789})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Llama-2-7b-hf-luis_tokenizer/tokenizer_config.json',\n",
       " 'Llama-2-7b-hf-luis_tokenizer/special_tokens_map.json',\n",
       " 'Llama-2-7b-hf-luis_tokenizer/tokenizer.model',\n",
       " 'Llama-2-7b-hf-luis_tokenizer/added_tokens.json',\n",
       " 'Llama-2-7b-hf-luis_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(new_model + \"_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
